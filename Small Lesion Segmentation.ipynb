{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "Fnaghman_Assignment-class_imbalance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HVTlhYeaP6m"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "In this assignment we will create a model for segmentation of enhancing tumor from brain MR images using custom loss function modifications to account for class imbalance.\n",
        "\n",
        "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A99BsBUaP6p"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Once complete, the following items must be submitted:\n",
        "\n",
        "* final `*.ipynb` notebook\n",
        "* final trained `*.hdf5` model file\n",
        "* final compiled `*.csv` file with performance statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56d3oMiMw8Wm"
      },
      "source": [
        "# Google Colab\n",
        "\n",
        "The following lines of code will configure your Google Colab environment for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OESWkE6gaP6q"
      },
      "source": [
        "### Enable GPU runtime\n",
        "\n",
        "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
        "\n",
        "```\n",
        "Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEVIS8SIaP6q"
      },
      "source": [
        "### Select Tensorflow library version\n",
        "\n",
        "This tutorial will use the Tensorflow 2.1 library. Use the following line of code to select and download this specific version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8xj40VLaP6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f00399-b7c3-4291-c623-1d8e7e3b800f"
      },
      "source": [
        "# --- Download Tensorflow 2.x (only in Google Colab)\n",
        "% pip install tensorflow-gpu==2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/e8/56ecca076108302a0bc34d509dc891086455ac31895843403ef0a71d0497/tensorflow_gpu-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.32.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.1.2)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (0.8.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.4.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (56.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.30.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.1) (2.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (4.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.7.4.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.4.8)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=0fad93dbe04c2688c8e711b54c76a7abdff3b3d2cd7eb3242c278e9e6a7c9e49\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, keras-applications, tensorflow-gpu\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6sHSq1naP6r"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu1ZvDSQaP6r"
      },
      "source": [
        "### Jarvis library\n",
        "\n",
        "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEcXyf3PaP6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84ad159-efd8-4b1f-9f00-32cebf237009"
      },
      "source": [
        "# --- Install jarvis (only in Google Colab or local runtime)\n",
        "% pip install jarvis-md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jarvis-md\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/8c/c0e9a5cc4840e50d0743824996f84a95922c4e21a71a991572323328df9e/jarvis_md-0.0.1a14-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (3.2.2)\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (2.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->jarvis-md) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->jarvis-md) (2018.9)\n",
            "Installing collected packages: pyyaml, jarvis-md\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed jarvis-md-0.0.1a14 pyyaml-5.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJA7WEaLaP6s"
      },
      "source": [
        "### Imports\n",
        "\n",
        "Use the following lines to import any additional needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIacWXNiaP6s"
      },
      "source": [
        "import numpy as np, pandas as pd\n",
        "from tensorflow import losses, optimizers\n",
        "from tensorflow.keras import Input, Model, models, layers, metrics\n",
        "from jarvis.train import datasets, custom\n",
        "from jarvis.train.client import Client\n",
        "from jarvis.utils.general import overload, tools as jtools\n",
        "from jarvis.utils.display import imshow"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naTWtjOwaP6s"
      },
      "source": [
        "# Data\n",
        "\n",
        "The data used in this tutorial will consist of brain tumor MRI exams derived from the MICCAI Brain Tumor Segmentation Challenge (BRaTS). More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/. Each single 2D slice will consist of one of four different sequences (T2, FLAIR, T1 pre-contrast and T1 post-contrast). In this exercise, we will use this dataset to derive a model for slice-by-slice tumor segmentation. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/mr_brats_2020`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJe57mhvaP6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d85fe515-35ce-410a-dee5-01c861253013"
      },
      "source": [
        "# --- Download dataset\n",
        "datasets.download(name='mr/brats-2020-mip')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-25 05:16:49 ] [====================] 100.000% : Extracting archive (0000750 / 0000750) "
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': '/data/raw/mr_brats_2020', 'data': '/data/raw/mr_brats_2020'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0ud-eruaP6t"
      },
      "source": [
        "# Training\n",
        "\n",
        "In order to create a high sensitivity classifier for pnuemonia, the following stratgies should be implemented in this assigment:\n",
        "\n",
        "* stratified sampling, and\n",
        "* pixel-level class weights, or\n",
        "* pixel-level masked loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-le70hWpaP6t"
      },
      "source": [
        "### Stratified Sampling\n",
        "\n",
        "Use the following code block to define a custom configuration dictionary to increase the sampling distribution of enhancing tumor (`lbl-mip-03`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL5DO5-AaP6t"
      },
      "source": [
        "# --- Configs dict to implement stratified sampling\n",
        "configs = {\n",
        "    'batch': {'size': 8},\n",
        "    'sampling': {\n",
        "        'lbl-mip-00': 0.3,\n",
        "        'lbl-mip-01': 0.1,\n",
        "        'lbl-mip-02': 0.1,\n",
        "        'lbl-mip-03': 0.5}}\n",
        "\n",
        "# --- Prepare generators\n",
        "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-mip', keyword='mip*vox', configs=configs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEv5R5Lrknld"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14GsK9a_aP6t"
      },
      "source": [
        "### Create custom generators\n",
        "\n",
        "*Hint*: Ensure that a combination of class weights and/or masked loss is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW3jErlhaP6t"
      },
      "source": [
        "def CustomGenerator(G):\n",
        "    \n",
        "    for xs, ys in G:\n",
        "        \n",
        "        # --- Define msk\n",
        "        xs['msk'] = np.zeros(ys['tumor'].shape, dtype='float32')\n",
        "        xs['msk'][ys['tumor'] == 3] = 0.5\n",
        "        \n",
        "        # --- Binarize ys\n",
        "        ys['tumor'] = ys['tumor'] == 3\n",
        "        ys['tumor'] = ys['tumor'].astype('uint8')\n",
        "        \n",
        "        yield xs, ys"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LFg2bYgCZu4"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjFpQ096dT0a"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMzcKhcvaP6u"
      },
      "source": [
        "### Create inputs\n",
        "\n",
        "*Hint*: Ensure that both the standard `dat` input as well as the addition `msk` input is accounted for in the `inputs` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CVerqDGaP6u"
      },
      "source": [
        "inputs = client.get_inputs(Input)\n",
        "inputs['msk'] = Input(shape=(None, 240, 240, 1), dtype='float32', name='msk')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PlLrJxmaP6u"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OFvxRO7aP6u"
      },
      "source": [
        "# --- Define kwargs dictionary\n",
        "kwargs = {\n",
        "    'kernel_size': (1, 3, 3),\n",
        "    'padding': 'same'}\n",
        "\n",
        "# --- Define lambda functions\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.ReLU()(x)\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        "\n",
        "concat = lambda a, b : layers.Concatenate()([a, b])\n",
        "\n",
        "# --- Define stride-1, stride-2 blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGMtiYeObrLr"
      },
      "source": [
        "# --- Define contracting layers\n",
        "l1 = conv1(8, inputs['dat'])\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, conv2(48, l3))\n",
        "l5 = conv1(64, conv2(64, l4))\n",
        "\n",
        "# --- Define expanding layers\n",
        "l6  = tran2(48, l5)\n",
        "l7  = tran2(32, conv1(48, concat(l4, l6)))\n",
        "l8  = tran2(16, conv1(32, concat(l3, l7)))\n",
        "l9  = tran2(8,  conv1(16, concat(l2, l8)))\n",
        "l10 = conv1(8,  l9)\n",
        "\n",
        "# --- Create logits\n",
        "logits = {}\n",
        "logits['tumor'] = layers.Conv3D(filters=2, name='tumor', **kwargs)(l10)\n",
        "\n",
        "# --- Create model\n",
        "model = Model(inputs=inputs, outputs=logits)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqPIeIHqaP6u"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "*Hint*: Ensure that custom loss functions are used as described in the tutorial to properly adjust the loss function for weights and masks. In addition it may be useful to track metrics such as Dice score and sensitivity to gauge real time performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NALNQDtb2yv"
      },
      "source": [
        "def sce(weights, scale=1.0):\n",
        "\n",
        "    loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    def sce(y_true, y_pred):\n",
        "\n",
        "        return loss(y_true=y_true, y_pred=y_pred, sample_weight=weights) * scale\n",
        "\n",
        "    return sce"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKVOADdRb-WM"
      },
      "source": [
        "loss = {'tumor': custom.sce(inputs['msk'])}\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HErXQ7EzcELp"
      },
      "source": [
        "# --- Create metrics\n",
        "metrics = custom.dsc(weights=inputs['msk'])\n",
        "metrics += [custom.softmax_ce_sens(weights=inputs['msk'])]\n",
        "\n",
        "metrics = {'tumor': metrics}\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_tocB-PcMsa"
      },
      "source": [
        "# --- Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    experimental_run_tf_function=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDYwbPuLcSk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f39651b-58e5-493b-9a49-f91547756615"
      },
      "source": [
        "client.load_data_in_memory()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-25 05:17:46 ] [====================] 100.000% : Iterating | 000368    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaE_GVRzfmbS"
      },
      "source": [
        "def dice(y_true, y_pred, c=1, epsilon=1):\n",
        "    \"\"\"\n",
        "    Method to calculate the Dice score coefficient for given class\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (np.ndarray) y_true : ground-truth label\n",
        "      (np.ndarray) y_pred : predicted logits scores\n",
        "      (int)             c : class to calculate DSC on\n",
        "    \n",
        "    \"\"\"\n",
        "    assert y_true.ndim == y_pred.ndim\n",
        "\n",
        "    A = np.count_nonzero(y_true & y_pred) * 2\n",
        "    B = np.count_nonzero(y_true) + np.count_nonzero(y_pred) + epsilon\n",
        "    \n",
        "    return A / B "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2he4JQufoUV"
      },
      "source": [
        "def calculate_sens(pred, true, epsilon=1):\n",
        "    \"\"\"\n",
        "    Method to calculate sensitivity from pred and true masks\n",
        "    \n",
        "    \"\"\"\n",
        "    truePositve=(pred==1) & (true==1)\n",
        "    groundTruth= (true==1)\n",
        "\n",
        "    return (truePositve.sum()+epsilon)/(groundTruth.sum()+epsilon)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGCQKmwfqfE"
      },
      "source": [
        "from tensorflow.keras import callbacks  \n",
        "tensorboard_callback = callbacks.TensorBoard('./logs')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q-f9heHaP6u"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Use the following cell block to train your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhVwTDFWaP6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f99bd1-e19a-4de8-fb6b-0f9a1d019814"
      },
      "source": [
        "gen_train_custom=CustomGenerator(gen_train)\n",
        "gen_valid_custom=CustomGenerator(gen_valid)\n",
        "\n",
        "model.fit(\n",
        "    x=gen_train_custom, \n",
        "    steps_per_epoch=50, \n",
        "    epochs=20,\n",
        "    validation_data=gen_valid_custom,\n",
        "    validation_steps=50,\n",
        "    validation_freq=4,\n",
        "    use_multiprocessing=True,\n",
        "    callbacks=[tensorboard_callback])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/20\n",
            " 1/50 [..............................] - ETA: 11:25 - loss: 0.0069 - dsc_1: 0.0528 - softmax_ce_sens: 0.0271WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.310338). Check your callbacks.\n",
            "50/50 [==============================] - 20s 400ms/step - loss: 0.0018 - dsc_1: 0.3129 - softmax_ce_sens: 0.2029\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 7s 147ms/step - loss: 6.7474e-04 - dsc_1: 0.7702 - softmax_ce_sens: 0.6744\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 7s 149ms/step - loss: 2.0047e-04 - dsc_1: 0.9303 - softmax_ce_sens: 0.9223\n",
            "Epoch 4/20\n",
            "49/50 [============================>.] - ETA: 0s - loss: 5.1140e-05 - dsc_1: 0.9564 - softmax_ce_sens: 0.9564WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 1.2741e-04 - dsc_1: 0.9182 - softmax_ce_sens: 0.9188\n",
            "50/50 [==============================] - 16s 319ms/step - loss: 5.0638e-05 - dsc_1: 0.9573 - softmax_ce_sens: 0.9572 - val_loss: 1.2741e-04 - val_dsc_1: 0.9182 - val_softmax_ce_sens: 0.9188\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 7s 137ms/step - loss: 2.6361e-05 - dsc_1: 0.8780 - softmax_ce_sens: 0.8787\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 8s 150ms/step - loss: 2.4436e-05 - dsc_1: 0.9276 - softmax_ce_sens: 0.9350\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 8s 151ms/step - loss: 1.6613e-05 - dsc_1: 0.9584 - softmax_ce_sens: 0.9591\n",
            "Epoch 8/20\n",
            "49/50 [============================>.] - ETA: 0s - loss: 1.3958e-05 - dsc_1: 0.9768 - softmax_ce_sens: 0.9795WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 8s 157ms/step - loss: 1.5911e-05 - dsc_1: 0.9185 - softmax_ce_sens: 0.9195\n",
            "50/50 [==============================] - 15s 306ms/step - loss: 1.3932e-05 - dsc_1: 0.9772 - softmax_ce_sens: 0.9799 - val_loss: 1.5911e-05 - val_dsc_1: 0.9185 - val_softmax_ce_sens: 0.9195\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 1.0292e-05 - dsc_1: 0.9392 - softmax_ce_sens: 0.9394\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 8s 150ms/step - loss: 6.1444e-06 - dsc_1: 0.9793 - softmax_ce_sens: 0.9800\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 8s 150ms/step - loss: 5.4457e-06 - dsc_1: 0.9390 - softmax_ce_sens: 0.9399\n",
            "Epoch 12/20\n",
            "49/50 [============================>.] - ETA: 0s - loss: 5.4906e-06 - dsc_1: 0.9303 - softmax_ce_sens: 0.9384WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 8s 156ms/step - loss: 7.1149e-06 - dsc_1: 0.9186 - softmax_ce_sens: 0.9196\n",
            "50/50 [==============================] - 15s 305ms/step - loss: 5.3808e-06 - dsc_1: 0.9295 - softmax_ce_sens: 0.9396 - val_loss: 7.1149e-06 - val_dsc_1: 0.9186 - val_softmax_ce_sens: 0.9196\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 7s 138ms/step - loss: 5.4355e-06 - dsc_1: 0.9153 - softmax_ce_sens: 0.9199\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 7s 149ms/step - loss: 2.5737e-06 - dsc_1: 0.9166 - softmax_ce_sens: 0.9169\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 7s 148ms/step - loss: 1.4804e-06 - dsc_1: 0.8572 - softmax_ce_sens: 0.8600\n",
            "Epoch 16/20\n",
            "49/50 [============================>.] - ETA: 0s - loss: 1.9223e-06 - dsc_1: 0.8734 - softmax_ce_sens: 0.8776WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 8s 157ms/step - loss: 4.1023e-06 - dsc_1: 0.9187 - softmax_ce_sens: 0.9198\n",
            "50/50 [==============================] - 15s 305ms/step - loss: 1.9763e-06 - dsc_1: 0.8760 - softmax_ce_sens: 0.8800 - val_loss: 4.1023e-06 - val_dsc_1: 0.9187 - val_softmax_ce_sens: 0.9198\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 1.4218e-06 - dsc_1: 0.8778 - softmax_ce_sens: 0.8800\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 8s 151ms/step - loss: 2.9842e-06 - dsc_1: 0.9952 - softmax_ce_sens: 0.9999\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 7s 150ms/step - loss: 2.1830e-06 - dsc_1: 0.8969 - softmax_ce_sens: 0.9000\n",
            "Epoch 20/20\n",
            "49/50 [============================>.] - ETA: 0s - loss: 1.7075e-06 - dsc_1: 0.9575 - softmax_ce_sens: 0.9592WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 8s 158ms/step - loss: 2.4308e-06 - dsc_1: 0.9187 - softmax_ce_sens: 0.9198\n",
            "50/50 [==============================] - 15s 306ms/step - loss: 1.6965e-06 - dsc_1: 0.9583 - softmax_ce_sens: 0.9600 - val_loss: 2.4308e-06 - val_dsc_1: 0.9187 - val_softmax_ce_sens: 0.9198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe1815aef90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LlpLzGPaP6v"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Based on the tutorial discussion, use the following cells to calculate model performance. The following metrics should be calculated:\n",
        "\n",
        "* pixel-wise sensitivity (mean, median, 25th percentile, 75th percentile)\n",
        "* Dice score coefficient (mean, median, 25th percentile, 75th percentile)\n",
        "\n",
        "### Performance\n",
        "\n",
        "The following minimum performance metrics must be met for full credit:\n",
        "\n",
        "* median pixel-wise sensitivity: >0.65\n",
        "* median Dice score coefficient: >0.65"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgZSU7sEaP6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390a6801-7e45-4e69-e50a-9d30e53346e7"
      },
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "custom_train= CustomGenerator(test_train)\n",
        "test_valid=CustomGenerator(test_valid)\n",
        "\n",
        "dsc=[]\n",
        "sens=[]\n",
        "for x, y in test_valid:\n",
        "    \n",
        "    # --- Create prediction\n",
        "    logits = model.predict(x)\n",
        "\n",
        "    pred = np.argmax(logits[0], axis=-1)\n",
        " \n",
        "    # --- Clean up pred using mask\n",
        "    pred[x['msk'][0, ..., 0] == 0] = 0\n",
        "\n",
        "    \n",
        "    # --- Calculate Dice\n",
        "    dsc.append(dice(y['tumor'][0,...,0], pred, c=1))\n",
        "\n",
        "\n",
        "    \n",
        "    # --- Calculate sens\n",
        "    sens.append(calculate_sens(pred=pred, true=y['tumor'][0,...,0]))\n",
        "\n",
        "dsc=np.array(dsc)\n",
        "sens=np.array(sens)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-25 05:24:37 ] [====================] 100.000% : Iterating | 000074    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2tqcETKaP6v"
      },
      "source": [
        "### Results\n",
        "\n",
        "When ready, create a `*.csv` file with your compiled **validation** cohort sensitivity and Dice score statistics. There is no need to submit training performance accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_orgJi6aaP6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "40973ec7-24a1-4aba-8500-595134d2ec88"
      },
      "source": [
        "df = pd.DataFrame(index=np.arange(dsc.size))\n",
        "df['dice'] = dsc\n",
        "df['sens'] = sens\n",
        "\n",
        "stats=pd.DataFrame(columns=['Median', 'Mean','25th percentile', '75th Percentile'])\n",
        "stats.loc['Dice']=df['dice'].median(), df['dice'].mean(), df['dice'].quantile(q=0.25), df['dice'].quantile(q=0.75)\n",
        "stats.loc['Sensitivity']=df['sens'].median(), df['sens'].mean(), df['sens'].quantile(0.25), df['sens'].quantile(0.75)\n",
        "stats\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Median</th>\n",
              "      <th>Mean</th>\n",
              "      <th>25th percentile</th>\n",
              "      <th>75th Percentile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Dice</th>\n",
              "      <td>0.999879</td>\n",
              "      <td>0.918608</td>\n",
              "      <td>0.99973</td>\n",
              "      <td>0.999946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sensitivity</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999872</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Median      Mean  25th percentile  75th Percentile\n",
              "Dice         0.999879  0.918608          0.99973         0.999946\n",
              "Sensitivity  1.000000  0.999872          1.00000         1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCw1N8bGC4yV",
        "outputId": "edf7c1f8-af54-422c-e3f1-9f110c78a074"
      },
      "source": [
        "print(df['dice'].describe())\n",
        "print(df['sens'].describe())\n",
        "\n",
        "df.to_csv('./results.csv')\n",
        "stats.to_csv('./stats.csv')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    74.000000\n",
            "mean      0.918608\n",
            "std       0.274731\n",
            "min       0.000000\n",
            "25%       0.999730\n",
            "50%       0.999879\n",
            "75%       0.999946\n",
            "max       0.999979\n",
            "Name: dice, dtype: float64\n",
            "count    74.000000\n",
            "mean      0.999872\n",
            "std       0.000980\n",
            "min       0.991576\n",
            "25%       1.000000\n",
            "50%       1.000000\n",
            "75%       1.000000\n",
            "max       1.000000\n",
            "Name: sens, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIzrwFbhaP6w"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Use the following line to save your model for submission (in Google Colab this should save your model file into your personal Google Drive):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUODM5dAaP6w"
      },
      "source": [
        "# --- Serialize a model\n",
        "model.save('./class_imbalance.hdf5')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "q-IDXKjRF16l",
        "outputId": "9b0af048-bba3-4ed4-f0f5-bd28d10b5b03"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-kt6lXiaP6w"
      },
      "source": [
        "### Canvas\n",
        "\n",
        "Once you have completed this assignment, download the necessary files from Google Colab and your Google Drive. You will then need to submit the following items:\n",
        "\n",
        "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
        "* final (results) spreadsheet: `[UCInetID]_results.csv`\n",
        "* final (trained) model: `[UCInetID]_model.hdf5`\n",
        "\n",
        "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooke file would be submitted under the name `panteater_notebook.ipynb`, his spreadshhet would be submitted under the name `panteater_results.csv` and and his model file would be submitted under the name `panteater_model.hdf5`."
      ]
    }
  ]
}