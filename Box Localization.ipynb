{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "assignment.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-FZE-V_i9qP"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "In this assignment we will train a box localization algorithm derived from RetinaNet to perform identification of brain tumors on MRI. The algorithm will be implemented using a feature pyramid network backbone. Accuracy will be calculated based on median IoU performance against ground-truth masks.\n",
        "\n",
        "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrUCbzhVi9qY"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Once complete, the following items must be submitted:\n",
        "\n",
        "* final `*.ipynb` notebook (push to https://github.com/[username]/cs190/cnn/assignment.ipynb)\n",
        "* final trained `*.hdf5` model file\n",
        "* final compiled `*.csv` file with performance statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56d3oMiMw8Wm"
      },
      "source": [
        "# Google Colab\n",
        "\n",
        "The following lines of code will configure your Google Colab environment for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anzmz1rwi9qZ"
      },
      "source": [
        "### Enable GPU runtime\n",
        "\n",
        "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
        "\n",
        "```\n",
        "Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmkJcyTTi9qZ"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM5-HJRDi9qa"
      },
      "source": [
        "### Tensorflow 2.1\n",
        "\n",
        "This tutorial specifically requires the use of Tensorflow 2.1 for implemention of custom (weighted) loss functions. Use the following command to install this specific library version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3bpzrbGi9qa",
        "outputId": "6c6a445f-63bf-4055-999a-f600c09d4f6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "% pip install tensorflow-gpu==2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/e8/56ecca076108302a0bc34d509dc891086455ac31895843403ef0a71d0497/tensorflow_gpu-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 19kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (0.8.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.4.1)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (3.12.4)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.12.1)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-gpu==2.1) (56.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.1) (2.10.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.30.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (4.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=02993a8df4576df7b819a7b3efacee54e86e3dfcde58797c9a44c45b31a0dc8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, keras-applications, gast, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_dofMxYi9qb"
      },
      "source": [
        "### Jarvis library\n",
        "\n",
        "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqmtxBAoi9qb",
        "outputId": "be309086-ac63-43d2-8f34-5eddb699f23b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# --- Install jarvis (only in Google Colab or local runtime)\n",
        "% pip install jarvis-md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jarvis-md\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/8c/c0e9a5cc4840e50d0743824996f84a95922c4e21a71a991572323328df9e/jarvis_md-0.0.1a14-py3-none-any.whl (81kB)\n",
            "\r\u001b[K     |████                            | 10kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 30kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 61kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 71kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (3.2.2)\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (2.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->jarvis-md) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->jarvis-md) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (3.0.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->jarvis-md) (1.15.0)\n",
            "Installing collected packages: pyyaml, jarvis-md\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed jarvis-md-0.0.1a14 pyyaml-5.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlaJS08ui9qb"
      },
      "source": [
        "### Imports\n",
        "\n",
        "Use the following lines to import any additional needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn8Nmh9ii9qc"
      },
      "source": [
        "import numpy as np, pandas as pd\n",
        "from tensorflow import losses, optimizers\n",
        "from tensorflow.keras import Input, Model, models, layers, metrics\n",
        "from jarvis.train import datasets, custom\n",
        "from jarvis.train.box import BoundingBox"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6gV5nQTi9qc"
      },
      "source": [
        "# Data\n",
        "\n",
        "The data used in this assignment will consist of brain tumor MRI exams derived from the MICCAI Brain Tumor Segmentation Challenge (BRaTS). More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/. Each single 2D slice will consist of one of four different sequences (T2, FLAIR, T1 pre-contrast and T1 post-contrast)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHKpKnD_i9qd"
      },
      "source": [
        "The following lines of code will:\n",
        "\n",
        "1. Download the dataset (if not already present) \n",
        "2. Prepare the necessary Python generators to iterate through dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0vrU3Qpi9qd",
        "outputId": "3f0071d5-ef97-4fae-db3e-a815f01a614e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# --- Download dataset\n",
        "datasets.download(name='mr/brats-2020-mip')\n",
        "\n",
        "# --- Prepare generators and model inputs\n",
        "configs = {'specs': {'ys': {'tumor': {'norms': {'clip': {'max': 1}}}}}}\n",
        "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-mip', keyword='mip*vox', configs=configs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-18 03:48:04 ] [====================] 100.000% : Extracting archive (0000750 / 0000750) "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ4_J3Yri9qd"
      },
      "source": [
        "# Training\n",
        "\n",
        "In this assignment we will train a box localization network for brain tumor segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD6V7ch_i9qd"
      },
      "source": [
        "### Define box parameters\n",
        "\n",
        "Use the following cell block to define your `BoundingBox` object as discussed in the tutorial. Feel free to optimize hyperparameter choices for grid size, anchor shapes, anchor aspect ratios, and anchor scales: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kHU9NeUi9qe"
      },
      "source": [
        "bb = BoundingBox(\n",
        "    image_shape=(240, 240),\n",
        "    classes=1,\n",
        "    c=[3, 4],\n",
        "    anchor_shapes=[32, 64],\n",
        "    anchor_scales=[0, 1, 2],\n",
        "    anchor_ratios=[0.5, 1, 2],\n",
        "    iou_upper=0.5,\n",
        "    iou_lower=0.2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it7jK8pKi9qe"
      },
      "source": [
        "### Define inputs\n",
        "\n",
        "Use the following cell block to define the nested generators needed to convert raw masks into bounding box ground-truth predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7HW-Spxi9qe"
      },
      "source": [
        "# --- Prepare generators\n",
        "gen_train, gen_valid = client.create_generators()\n",
        "gen_train, gen_valid = bb.create_generators(gen_train, gen_valid, msk='tumor')\n",
        "\n",
        "# --- Create inputs\n",
        "inputs = client.get_inputs(Input)\n",
        "inputs = bb.get_inputs(inputs, Input)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKx8UImsi9qf"
      },
      "source": [
        "### Define the model\n",
        "\n",
        "Use the following cell block to define your feature pyramid network backbone and RetinaNet classification / regression networks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqMJl-MVi9qf"
      },
      "source": [
        "\n",
        "# --- Define kwargs dictionary\n",
        "kwargs = {\n",
        "    'kernel_size': (1, 3, 3),\n",
        "    'padding': 'same'}\n",
        "\n",
        "# --- Define lambda functions\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.ReLU()(x)\n",
        "\n",
        "# --- Define stride-1, stride-2 blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
        "\n",
        "# --- Define contracting layers\n",
        "l1 = conv1(8, inputs['dat'])\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(24, conv2(24, l2))\n",
        "l4 = conv1(32, conv2(32, l3))\n",
        "l5 = conv1(48, conv2(48, l4))\n",
        "\n",
        "# --- Define zoom\n",
        "zoom = lambda x : layers.UpSampling3D(\n",
        "    size=(1, 2, 2))(x)\n",
        "\n",
        "\n",
        "# --- Define 1 x 1 x 1 projection\n",
        "proj = lambda filters, x : layers.Conv3D(\n",
        "    filters=filters,\n",
        "    strides=1,\n",
        "    kernel_size=(1, 1, 1),\n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "# --- Define expanding layers\n",
        "l6 = proj(64, l5)\n",
        "l7 = conv1(64, zoom(l6) + proj(64, l4))\n",
        "\n",
        "# --- Determine filter sizes\n",
        "logits = {}\n",
        "K = 1\n",
        "A = 9\n",
        "\n",
        "# --- C3\n",
        "c3_cls = conv1(64, conv1(64, l7))\n",
        "c3_reg = conv1(64, conv1(64, l7))\n",
        "logits['cls-c3'] = layers.Conv3D(filters=(A * K), name='cls-c3', **kwargs)(c3_cls)\n",
        "logits['reg-c3'] = layers.Conv3D(filters=(A * 4), name='reg-c3', **kwargs)(c3_reg)\n",
        "\n",
        "# --- C4\n",
        "c4_cls = conv1(64, conv1(64, l6))\n",
        "c4_reg = conv1(64, conv1(64, l6))\n",
        "logits['cls-c4'] = layers.Conv3D(filters=(A * K), name='cls-c4', **kwargs)(c4_cls)\n",
        "logits['reg-c4'] = layers.Conv3D(filters=(A * 4), name='reg-c4', **kwargs)(c4_reg)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=logits)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMX7AELFi9qf"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "Use the following cell block to compile your model. Recall the following requirements as described in the tutorial:\n",
        "\n",
        "* use of a focal sigmoid (binary) cross-entropy loss function for regression\n",
        "* use of a Huber loss function for classification\n",
        "* use of masked loss functions to ensure only relevant examples are used for training\n",
        "* use of appropriate metrics to track algorithm training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J83FrAGNi9qf"
      },
      "source": [
        "def focal_sigmoid_ce(weights=1.0, scale=1.0, gamma=2.0, alpha=0.25):\n",
        "    \"\"\"\n",
        "    Method to implement focal sigmoid (binary) cross-entropy loss\n",
        "\n",
        "    \"\"\"\n",
        "    def focal_sigmoid_ce(y_true, y_pred):\n",
        "\n",
        "        # --- Calculate standard cross entropy with alpha weighting\n",
        "        loss = tf.nn.weighted_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred, pos_weight=alpha)\n",
        "\n",
        "        # --- Calculate modulation to pos and neg labels \n",
        "        p = tf.math.sigmoid(y_pred)\n",
        "        modulation_pos = (1 - p) ** gamma\n",
        "        modulation_neg = p ** gamma\n",
        "\n",
        "        mask = tf.dtypes.cast(y_true, dtype=tf.bool)\n",
        "        modulation = tf.where(mask, modulation_pos, modulation_neg)\n",
        "\n",
        "        return tf.math.reduce_sum(modulation * loss * weights * scale)\n",
        "\n",
        "    return focal_sigmoid_ce"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4CJrA4PnaeB",
        "outputId": "d3af7d6b-d165-47a6-8e45-7bd0e7c0dbb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# --- Create sensivity and PPV metrics\n",
        "custom.sigmoid_ce_sens()\n",
        "custom.sigmoid_ce_ppv()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function jarvis.train.custom.custom.sigmoid_ce_ppv.<locals>.sigmoid_ce_ppv>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "burqQo2Enn-g"
      },
      "source": [
        "# --- Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss={\n",
        "        'cls-c3': custom.focal_sigmoid_ce(inputs['cls-c3-msk']),\n",
        "        'cls-c4': custom.focal_sigmoid_ce(inputs['cls-c4-msk']),\n",
        "        'reg-c3': custom.sl1(inputs['reg-c3-msk']),\n",
        "        'reg-c4': custom.sl1(inputs['reg-c4-msk']),\n",
        "        },\n",
        "    metrics={\n",
        "        'cls-c3': [custom.sigmoid_ce_sens(), custom.sigmoid_ce_ppv()],\n",
        "        'cls-c4': [custom.sigmoid_ce_sens(), custom.sigmoid_ce_ppv()]},\n",
        "    experimental_run_tf_function=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcS8nOmQi9qg"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Use the following cell block to train your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZxMvOd-i9qg",
        "outputId": "ed8439fe-c06b-4539-daf5-57f3ec3e0135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "client.load_data_in_memory()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-18 03:58:08 ] [====================] 100.000% : Iterating | 000368    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1W8vf4dnu-s",
        "outputId": "c8ede900-3386-4698-cb22-3cdf29edff6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# --- Train model\n",
        "model.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=500, \n",
        "    epochs=16,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=500,\n",
        "    validation_freq=4,\n",
        "    use_multiprocessing=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/16\n",
            "500/500 [==============================] - 106s 212ms/step - loss: 0.0189 - cls-c3_loss: 0.0080 - cls-c4_loss: 0.0107 - reg-c3_loss: 9.3513e-05 - reg-c4_loss: 7.0688e-05 - cls-c3_sigmoid_ce_sens: 0.0552 - cls-c3_sigmoid_ce_ppv: 0.2486 - cls-c4_sigmoid_ce_sens: 0.0610 - cls-c4_sigmoid_ce_ppv: 0.2073\n",
            "Epoch 2/16\n",
            "500/500 [==============================] - 95s 189ms/step - loss: 8.9238e-04 - cls-c3_loss: 3.9759e-04 - cls-c4_loss: 4.2626e-04 - reg-c3_loss: 5.0901e-05 - reg-c4_loss: 1.7633e-05 - cls-c3_sigmoid_ce_sens: 0.2473 - cls-c3_sigmoid_ce_ppv: 0.4530 - cls-c4_sigmoid_ce_sens: 0.3873 - cls-c4_sigmoid_ce_ppv: 0.4803\n",
            "Epoch 3/16\n",
            "500/500 [==============================] - 94s 189ms/step - loss: 6.3086e-04 - cls-c3_loss: 2.8982e-04 - cls-c4_loss: 2.7362e-04 - reg-c3_loss: 5.2839e-05 - reg-c4_loss: 1.4581e-05 - cls-c3_sigmoid_ce_sens: 0.3646 - cls-c3_sigmoid_ce_ppv: 0.3925 - cls-c4_sigmoid_ce_sens: 0.5295 - cls-c4_sigmoid_ce_ppv: 0.4388\n",
            "Epoch 4/16\n",
            "499/500 [============================>.] - ETA: 0s - loss: 4.9223e-04 - cls-c3_loss: 2.4278e-04 - cls-c4_loss: 1.9132e-04 - reg-c3_loss: 4.6295e-05 - reg-c4_loss: 1.1827e-05 - cls-c3_sigmoid_ce_sens: 0.4574 - cls-c3_sigmoid_ce_ppv: 0.3699 - cls-c4_sigmoid_ce_sens: 0.6227 - cls-c4_sigmoid_ce_ppv: 0.4073WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/16\n",
            "500/500 [==============================] - 89s 177ms/step - loss: 5.5052e-04 - cls-c3_loss: 2.2313e-04 - cls-c4_loss: 2.6457e-04 - reg-c3_loss: 4.9350e-05 - reg-c4_loss: 1.3468e-05 - cls-c3_sigmoid_ce_sens: 0.4695 - cls-c3_sigmoid_ce_ppv: 0.3331 - cls-c4_sigmoid_ce_sens: 0.5194 - cls-c4_sigmoid_ce_ppv: 0.4532\n",
            "500/500 [==============================] - 183s 367ms/step - loss: 4.9150e-04 - cls-c3_loss: 2.4243e-04 - cls-c4_loss: 1.9105e-04 - reg-c3_loss: 4.6207e-05 - reg-c4_loss: 1.1816e-05 - cls-c3_sigmoid_ce_sens: 0.4584 - cls-c3_sigmoid_ce_ppv: 0.3698 - cls-c4_sigmoid_ce_sens: 0.6234 - cls-c4_sigmoid_ce_ppv: 0.4077 - val_loss: 5.5052e-04 - val_cls-c3_loss: 2.2313e-04 - val_cls-c4_loss: 2.6457e-04 - val_reg-c3_loss: 4.9350e-05 - val_reg-c4_loss: 1.3468e-05 - val_cls-c3_sigmoid_ce_sens: 0.4695 - val_cls-c3_sigmoid_ce_ppv: 0.3331 - val_cls-c4_sigmoid_ce_sens: 0.5194 - val_cls-c4_sigmoid_ce_ppv: 0.4532\n",
            "Epoch 5/16\n",
            "500/500 [==============================] - 94s 189ms/step - loss: 4.2392e-04 - cls-c3_loss: 2.1620e-04 - cls-c4_loss: 1.5248e-04 - reg-c3_loss: 4.5042e-05 - reg-c4_loss: 1.0191e-05 - cls-c3_sigmoid_ce_sens: 0.5023 - cls-c3_sigmoid_ce_ppv: 0.3411 - cls-c4_sigmoid_ce_sens: 0.6871 - cls-c4_sigmoid_ce_ppv: 0.3866\n",
            "Epoch 6/16\n",
            "500/500 [==============================] - 94s 188ms/step - loss: 3.6355e-04 - cls-c3_loss: 1.9155e-04 - cls-c4_loss: 1.2369e-04 - reg-c3_loss: 3.9456e-05 - reg-c4_loss: 8.8459e-06 - cls-c3_sigmoid_ce_sens: 0.5629 - cls-c3_sigmoid_ce_ppv: 0.3274 - cls-c4_sigmoid_ce_sens: 0.7180 - cls-c4_sigmoid_ce_ppv: 0.3670\n",
            "Epoch 7/16\n",
            "500/500 [==============================] - 94s 189ms/step - loss: 3.3334e-04 - cls-c3_loss: 1.8056e-04 - cls-c4_loss: 1.0555e-04 - reg-c3_loss: 3.8784e-05 - reg-c4_loss: 8.4439e-06 - cls-c3_sigmoid_ce_sens: 0.5808 - cls-c3_sigmoid_ce_ppv: 0.3152 - cls-c4_sigmoid_ce_sens: 0.7419 - cls-c4_sigmoid_ce_ppv: 0.3385\n",
            "Epoch 8/16\n",
            "499/500 [============================>.] - ETA: 0s - loss: 3.0504e-04 - cls-c3_loss: 1.7143e-04 - cls-c4_loss: 8.6926e-05 - reg-c3_loss: 3.9264e-05 - reg-c4_loss: 7.4173e-06 - cls-c3_sigmoid_ce_sens: 0.5873 - cls-c3_sigmoid_ce_ppv: 0.3139 - cls-c4_sigmoid_ce_sens: 0.7616 - cls-c4_sigmoid_ce_ppv: 0.3242WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/16\n",
            "500/500 [==============================] - 88s 175ms/step - loss: 3.3610e-04 - cls-c3_loss: 1.6801e-04 - cls-c4_loss: 1.2070e-04 - reg-c3_loss: 3.6549e-05 - reg-c4_loss: 1.0846e-05 - cls-c3_sigmoid_ce_sens: 0.6666 - cls-c3_sigmoid_ce_ppv: 0.2233 - cls-c4_sigmoid_ce_sens: 0.7011 - cls-c4_sigmoid_ce_ppv: 0.3645\n",
            "500/500 [==============================] - 182s 364ms/step - loss: 3.0504e-04 - cls-c3_loss: 1.7136e-04 - cls-c4_loss: 8.6775e-05 - reg-c3_loss: 3.9507e-05 - reg-c4_loss: 7.4042e-06 - cls-c3_sigmoid_ce_sens: 0.5874 - cls-c3_sigmoid_ce_ppv: 0.3138 - cls-c4_sigmoid_ce_sens: 0.7621 - cls-c4_sigmoid_ce_ppv: 0.3244 - val_loss: 3.3610e-04 - val_cls-c3_loss: 1.6801e-04 - val_cls-c4_loss: 1.2070e-04 - val_reg-c3_loss: 3.6549e-05 - val_reg-c4_loss: 1.0846e-05 - val_cls-c3_sigmoid_ce_sens: 0.6666 - val_cls-c3_sigmoid_ce_ppv: 0.2233 - val_cls-c4_sigmoid_ce_sens: 0.7011 - val_cls-c4_sigmoid_ce_ppv: 0.3645\n",
            "Epoch 9/16\n",
            "500/500 [==============================] - 94s 187ms/step - loss: 3.2161e-04 - cls-c3_loss: 1.8412e-04 - cls-c4_loss: 8.9178e-05 - reg-c3_loss: 4.0777e-05 - reg-c4_loss: 7.5347e-06 - cls-c3_sigmoid_ce_sens: 0.5796 - cls-c3_sigmoid_ce_ppv: 0.3017 - cls-c4_sigmoid_ce_sens: 0.7702 - cls-c4_sigmoid_ce_ppv: 0.3156\n",
            "Epoch 10/16\n",
            "500/500 [==============================] - 94s 189ms/step - loss: 2.7689e-04 - cls-c3_loss: 1.6057e-04 - cls-c4_loss: 7.3184e-05 - reg-c3_loss: 3.6192e-05 - reg-c4_loss: 6.9426e-06 - cls-c3_sigmoid_ce_sens: 0.6308 - cls-c3_sigmoid_ce_ppv: 0.3027 - cls-c4_sigmoid_ce_sens: 0.8072 - cls-c4_sigmoid_ce_ppv: 0.2983\n",
            "Epoch 11/16\n",
            "500/500 [==============================] - 94s 188ms/step - loss: 2.5993e-04 - cls-c3_loss: 1.5141e-04 - cls-c4_loss: 6.6965e-05 - reg-c3_loss: 3.5075e-05 - reg-c4_loss: 6.4794e-06 - cls-c3_sigmoid_ce_sens: 0.6515 - cls-c3_sigmoid_ce_ppv: 0.2944 - cls-c4_sigmoid_ce_sens: 0.7948 - cls-c4_sigmoid_ce_ppv: 0.2872\n",
            "Epoch 12/16\n",
            "499/500 [============================>.] - ETA: 0s - loss: 2.6085e-04 - cls-c3_loss: 1.5249e-04 - cls-c4_loss: 6.5810e-05 - reg-c3_loss: 3.6440e-05 - reg-c4_loss: 6.1161e-06 - cls-c3_sigmoid_ce_sens: 0.6585 - cls-c3_sigmoid_ce_ppv: 0.2883 - cls-c4_sigmoid_ce_sens: 0.8290 - cls-c4_sigmoid_ce_ppv: 0.2738WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/16\n",
            "500/500 [==============================] - 88s 175ms/step - loss: 2.6952e-04 - cls-c3_loss: 1.4790e-04 - cls-c4_loss: 7.9567e-05 - reg-c3_loss: 3.4948e-05 - reg-c4_loss: 7.1072e-06 - cls-c3_sigmoid_ce_sens: 0.6927 - cls-c3_sigmoid_ce_ppv: 0.2241 - cls-c4_sigmoid_ce_sens: 0.8077 - cls-c4_sigmoid_ce_ppv: 0.2507\n",
            "500/500 [==============================] - 182s 365ms/step - loss: 2.6048e-04 - cls-c3_loss: 1.5226e-04 - cls-c4_loss: 6.5719e-05 - reg-c3_loss: 3.6379e-05 - reg-c4_loss: 6.1160e-06 - cls-c3_sigmoid_ce_sens: 0.6591 - cls-c3_sigmoid_ce_ppv: 0.2882 - cls-c4_sigmoid_ce_sens: 0.8292 - cls-c4_sigmoid_ce_ppv: 0.2737 - val_loss: 2.6952e-04 - val_cls-c3_loss: 1.4790e-04 - val_cls-c4_loss: 7.9567e-05 - val_reg-c3_loss: 3.4948e-05 - val_reg-c4_loss: 7.1072e-06 - val_cls-c3_sigmoid_ce_sens: 0.6927 - val_cls-c3_sigmoid_ce_ppv: 0.2241 - val_cls-c4_sigmoid_ce_sens: 0.8077 - val_cls-c4_sigmoid_ce_ppv: 0.2507\n",
            "Epoch 13/16\n",
            "500/500 [==============================] - 94s 188ms/step - loss: 2.6799e-04 - cls-c3_loss: 1.6117e-04 - cls-c4_loss: 6.0162e-05 - reg-c3_loss: 4.0579e-05 - reg-c4_loss: 6.0772e-06 - cls-c3_sigmoid_ce_sens: 0.6459 - cls-c3_sigmoid_ce_ppv: 0.2965 - cls-c4_sigmoid_ce_sens: 0.8050 - cls-c4_sigmoid_ce_ppv: 0.2698\n",
            "Epoch 14/16\n",
            "500/500 [==============================] - 94s 189ms/step - loss: 2.2866e-04 - cls-c3_loss: 1.4584e-04 - cls-c4_loss: 3.9585e-05 - reg-c3_loss: 3.7810e-05 - reg-c4_loss: 5.4227e-06 - cls-c3_sigmoid_ce_sens: 0.6506 - cls-c3_sigmoid_ce_ppv: 0.2824 - cls-c4_sigmoid_ce_sens: 0.8195 - cls-c4_sigmoid_ce_ppv: 0.2419\n",
            "Epoch 15/16\n",
            "500/500 [==============================] - 95s 190ms/step - loss: 2.1049e-04 - cls-c3_loss: 1.3793e-04 - cls-c4_loss: 3.4834e-05 - reg-c3_loss: 3.2297e-05 - reg-c4_loss: 5.4261e-06 - cls-c3_sigmoid_ce_sens: 0.6838 - cls-c3_sigmoid_ce_ppv: 0.2710 - cls-c4_sigmoid_ce_sens: 0.8750 - cls-c4_sigmoid_ce_ppv: 0.2338\n",
            "Epoch 16/16\n",
            "499/500 [============================>.] - ETA: 0s - loss: 2.4213e-04 - cls-c3_loss: 1.4283e-04 - cls-c4_loss: 5.5221e-05 - reg-c3_loss: 3.8402e-05 - reg-c4_loss: 5.6762e-06 - cls-c3_sigmoid_ce_sens: 0.6657 - cls-c3_sigmoid_ce_ppv: 0.2711 - cls-c4_sigmoid_ce_sens: 0.8098 - cls-c4_sigmoid_ce_ppv: 0.2559WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/16\n",
            "500/500 [==============================] - 88s 175ms/step - loss: 6.2324e-04 - cls-c3_loss: 1.9195e-04 - cls-c4_loss: 3.8752e-04 - reg-c3_loss: 3.6235e-05 - reg-c4_loss: 7.5300e-06 - cls-c3_sigmoid_ce_sens: 0.6776 - cls-c3_sigmoid_ce_ppv: 0.2394 - cls-c4_sigmoid_ce_sens: 0.8814 - cls-c4_sigmoid_ce_ppv: 0.1745\n",
            "500/500 [==============================] - 182s 364ms/step - loss: 2.4328e-04 - cls-c3_loss: 1.4361e-04 - cls-c4_loss: 5.5316e-05 - reg-c3_loss: 3.8689e-05 - reg-c4_loss: 5.6668e-06 - cls-c3_sigmoid_ce_sens: 0.6653 - cls-c3_sigmoid_ce_ppv: 0.2710 - cls-c4_sigmoid_ce_sens: 0.8101 - cls-c4_sigmoid_ce_ppv: 0.2556 - val_loss: 6.2324e-04 - val_cls-c3_loss: 1.9195e-04 - val_cls-c4_loss: 3.8752e-04 - val_reg-c3_loss: 3.6235e-05 - val_reg-c4_loss: 7.5300e-06 - val_cls-c3_sigmoid_ce_sens: 0.6776 - val_cls-c3_sigmoid_ce_ppv: 0.2394 - val_cls-c4_sigmoid_ce_sens: 0.8814 - val_cls-c4_sigmoid_ce_ppv: 0.1745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe2ea50e790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbtnzSdcoumk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzeB5qCni9qg"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Based on the tutorial discussion, use the following cells to calculate model performance. The following metrics should be calculated:\n",
        "\n",
        "* median IoU\n",
        "* 25th percentile IoU\n",
        "* 75th percentile IoU\n",
        "\n",
        "### Performance\n",
        "\n",
        "The following minimum performance metrics must be met for full credit:\n",
        "\n",
        "* median IoU: >0.50\n",
        "* 25th percentile IoU: >0.40\n",
        "* 75th percentile IoU: >0.60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4aT8fbvi9qh",
        "outputId": "eeaaa8f7-72e7-4c66-8734-389b6a75c01f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "test_train, test_valid = bb.create_generators(test_train, test_valid, msk='tumor')\n",
        "\n",
        "ious = {\n",
        "    'med': [],\n",
        "    'p25': [],\n",
        "    'p75': []}\n",
        "\n",
        "for x, y in test_valid:\n",
        "    \n",
        "    # --- Predict\n",
        "    box = model.predict(x)\n",
        "    if type(box) is list:\n",
        "        box = {k: l for k, l in zip(model.output_names, box)}\n",
        "        \n",
        "    # --- Convert predictions to anchors\n",
        "    anchors_pred, _ = bb.convert_box_to_anc(box, iou_nms=0.5)\n",
        "    \n",
        "    # --- Convert ground-truth to anchors\n",
        "    anchors_true, _ = bb.convert_box_to_anc(y)\n",
        "    \n",
        "    # --- Calculate IoUs\n",
        "    curr = []\n",
        "    for pred, true in zip(anchors_pred, anchors_true):\n",
        "        for p in pred:\n",
        "            iou = bb.calculate_ious(box=p, anchors=true)\n",
        "            if iou.size > 0:\n",
        "                curr.append(np.max(iou))\n",
        "            else: \n",
        "                curr.append(0)\n",
        "    \n",
        "    if len(curr) == 0:\n",
        "        curr = [0]\n",
        "        \n",
        "    ious['med'].append(np.median(curr))\n",
        "    ious['p25'].append(np.percentile(curr, 25))\n",
        "    ious['p75'].append(np.percentile(curr, 75))\n",
        "    \n",
        "ious = {k: np.array(v) for k, v in ious.items()}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-18 04:39:16 ] [====================] 100.000% : Iterating | 000074    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3uXgkhapHUb",
        "outputId": "92359e5c-4c24-4b2d-e46e-b2693f0add68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# --- Define columns\n",
        "df = pd.DataFrame(index=np.arange(ious['med'].size))\n",
        "df['iou_median'] = ious['med']\n",
        "df['iou_p-25th'] = ious['p25']\n",
        "df['iou_p-75th'] = ious['p75']\n",
        "\n",
        "df.to_csv('./results.csv')\n",
        "# --- Print accuracy\n",
        "print(df['iou_median'].median())\n",
        "print(df['iou_p-25th'].median())\n",
        "print(df['iou_p-75th'].median())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5794649124145508\n",
            "0.44157037883996964\n",
            "0.7426700443029404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2DTVu2Ci9qh"
      },
      "source": [
        "### Results\n",
        "\n",
        "When ready, create a `*.csv` file with your compiled **validation** cohort IoU statistics. There is no need to submit training performance accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xqXgdsOi9qi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6jVFSBpi9qi"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Use the following line to save your model for submission (in Google Colab this should save your model file into your personal Google Drive):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT3hH3AAi9qj"
      },
      "source": [
        "# --- Serialize a model\n",
        "model.save('./box_localization.hdf5')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvNTFgR_i9qj"
      },
      "source": [
        "### Canvas\n",
        "\n",
        "Once you have completed this assignment, download the necessary files from Google Colab and your Google Drive. You will then need to submit the following items:\n",
        "\n",
        "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
        "* final (results) spreadsheet: `[UCInetID]_results.csv`\n",
        "* final (trained) model: `[UCInetID]_model.hdf5`\n",
        "\n",
        "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooke file would be submitted under the name `panteater_notebook.ipynb`, his spreadshhet would be submitted under the name `panteater_results.csv` and and his model file would be submitted under the name `panteater_model.hdf5`."
      ]
    }
  ]
}