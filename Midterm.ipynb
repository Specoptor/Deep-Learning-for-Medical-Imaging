{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "Midterm assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t8-WpJ4Mx4e"
      },
      "source": [
        "# Midterm\n",
        "\n",
        "The midterm project will consist of a comparison between several CNN architectures for tumor segmentation. The goal is both to create a high-performing algorithm for the target task, as well as to analyze performance across several different architecture permutations. In total, three different network designs will be tested. As each model is built and trained, ensure to serialize the final model `*.hdf5` file before moving to the next iteration.\n",
        "\n",
        "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIVx3gZ4Mx4l"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Once complete, the following items must be submitted:\n",
        "\n",
        "* final `*.ipynb` notebook\n",
        "* final trained `*.hdf5` model files for all three models\n",
        "* final compiled `*.csv` file with performance statistics across the different architectures\n",
        "* final 1-page write-up with methods and results of experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56d3oMiMw8Wm"
      },
      "source": [
        "# Google Colab\n",
        "\n",
        "The following lines of code will configure your Google Colab environment for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmoSvEoJMx4m"
      },
      "source": [
        "### Enable GPU runtime\n",
        "\n",
        "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
        "\n",
        "```\n",
        "Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygt3yWF2Mx4n"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3fKRSBzMx4n"
      },
      "source": [
        "### Jarvis library\n",
        "\n",
        "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWNl0Ih0Mx4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1cb8ab-e7b3-42be-e938-ec2c87e80f44"
      },
      "source": [
        "# --- Install jarvis (only in Google Colab or local runtime)\n",
        "!pip install jarvis-md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jarvis-md in /usr/local/lib/python3.7/dist-packages (0.0.1a14)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (5.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (2.23.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (2.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.1.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->jarvis-md) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->jarvis-md) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o90_M-kuMx4o"
      },
      "source": [
        "### Imports\n",
        "\n",
        "Use the following lines to import any additional needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpu8XEFEMx4o"
      },
      "source": [
        "import os, numpy as np, pandas as pd\n",
        "from tensorflow import losses, optimizers\n",
        "from tensorflow.keras import Input, Model, models, layers\n",
        "from jarvis.train import datasets, custom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d40hWCYKMx4o"
      },
      "source": [
        "# Data\n",
        "\n",
        "As in the tutorial, data for this assignment will consist of tumor MRI exams. The following lines of code will download the dataset (if not already present). Since the algorithms below may require slightly different model inputs, the required generators and inputs will be defined dyanically in the code blocks later in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHUa8bFPMx4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcd47b9-0a33-494b-88f2-c50a2f5ccfea"
      },
      "source": [
        "# --- Download dataset\n",
        "datasets.download(name='mr/brats-2020-mip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': '/data/raw/mr_brats_2020', 'data': '/data/raw/mr_brats_2020'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3mqey1xMx4p"
      },
      "source": [
        "# Training\n",
        "\n",
        "A total of three different network architectures will be tested. The goal is to compare the incremental benefit of several design choices. After building and training each model to convergence, do not forget to save each model as a separate `*.hdf5` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08jwO921Mx4p"
      },
      "source": [
        "## 1. 2D U-Net\n",
        "\n",
        "In this algorithm a standard 2D U-Net architecture will be used to perform tumor segmentation. This network is **identical** to the baseline architectures presented in the week 5 and week 6 tutorials. The algorithm input will include an entire full field-of-view `240 x 240` resolution 2D slice from a brain MRI across 4 different modalities (channels). Key customizations to the standard U-Net architecture that should be implemented (as in the week 5 and week 6 tutorials) include:\n",
        "\n",
        "* same padding (vs. valid padding)\n",
        "* strided convolutions (vs. max-pooling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoGYKkbxMx4q"
      },
      "source": [
        "### Create generators and inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtLZlovTMx4q"
      },
      "source": [
        "\n",
        "configs = {'specs': {'ys': {'tumor': {'norms': {'clip': {'max': 1}}}}}}\n",
        "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-mip', keyword='mip*vox', configs=configs)\n",
        "inputs = client.get_inputs(Input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gnd_rvtMx4q"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3accNvoBMx4q"
      },
      "source": [
        "# 3x3 FILTERS | BATCH NORM | ReLU | STRITED CONV\n",
        "kwargs = {\n",
        "    'kernel_size': (1, 3, 3),\n",
        "    'padding': 'same',\n",
        "    'kernel_initializer': 'he_normal'}\n",
        " \n",
        "# --- Define block components\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        " \n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.ReLU()(x)\n",
        " \n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))\n",
        " \n",
        "# --- Define contracting layers\n",
        "l1 = conv1(8, inputs['dat'])\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, conv2(48, l3))\n",
        "l5 = conv1(64, conv2(64, l4))\n",
        " \n",
        "# --- Define single transpose\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        " \n",
        "# --- Define transpose block\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))\n",
        " \n",
        "# --- Define expanding layers\n",
        "l6 = tran2(48, l5)\n",
        " \n",
        "l7 = tran2(32, conv1(48, l6+conv1(48, l4)))\n",
        "l8 = tran2(16, conv1(32, l7+conv1(32, conv1(32, l3))))\n",
        "l9 = tran2(8,  conv1(16, l8+conv1(16, conv1(16, conv1(16, l2)))))\n",
        "l10 = conv1(8, conv1(8, l9+conv1(8, conv1(8, conv1(8, l1)))))\n",
        "# --- Create logits\n",
        "logits = {}\n",
        "logits['tumor'] = layers.Conv3D(filters=2, name='tumor', **kwargs)(l10)\n",
        " \n",
        "# --- Create model\n",
        "model = Model(inputs=inputs, outputs=logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6ZRpm3cMx4r"
      },
      "source": [
        "### Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O5nBl3gMx4r"
      },
      "source": [
        "# --- Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.005),\n",
        "    loss={'tumor': losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
        "    metrics={'tumor': custom.dsc(cls=1)},\n",
        "    experimental_run_tf_function=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4vQeg8L-lSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1293fde1-f768-42a7-c9ee-bd659e605cad"
      },
      "source": [
        "model.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=500, \n",
        "    epochs=9,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=500,\n",
        "    validation_freq=3,\n",
        "    use_multiprocessing=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 197s 361ms/step - loss: 0.0758 - dsc_1: 0.5432\n",
            "Epoch 2/9\n",
            "500/500 [==============================] - 183s 366ms/step - loss: 0.0188 - dsc_1: 0.7089\n",
            "Epoch 3/9\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0137 - dsc_1: 0.7811WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 338s 676ms/step - loss: 0.0137 - dsc_1: 0.7811 - val_loss: 0.0132 - val_dsc_1: 0.8064\n",
            "Epoch 4/9\n",
            "500/500 [==============================] - 181s 362ms/step - loss: 0.0125 - dsc_1: 0.7863\n",
            "Epoch 5/9\n",
            "500/500 [==============================] - 182s 364ms/step - loss: 0.0109 - dsc_1: 0.8007\n",
            "Epoch 6/9\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0112 - dsc_1: 0.8141WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 338s 676ms/step - loss: 0.0112 - dsc_1: 0.8141 - val_loss: 0.0129 - val_dsc_1: 0.8165\n",
            "Epoch 7/9\n",
            "500/500 [==============================] - 182s 365ms/step - loss: 0.0104 - dsc_1: 0.8207\n",
            "Epoch 8/9\n",
            "500/500 [==============================] - 184s 367ms/step - loss: 0.0101 - dsc_1: 0.8239\n",
            "Epoch 9/9\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0105 - dsc_1: 0.8261WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 340s 680ms/step - loss: 0.0105 - dsc_1: 0.8261 - val_loss: 0.0106 - val_dsc_1: 0.8191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb8df01c490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ_nQ9exEClW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcd789e-be1e-424c-9a3f-29fa60c935e7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "dat (InputLayer)                [(None, None, 240, 2 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d (Conv3D)                 (None, None, 240, 24 296         dat[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, 240, 24 32          conv3d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu (ReLU)                    (None, None, 240, 24 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_1 (Conv3D)               (None, None, 120, 12 1168        re_lu[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 120, 12 64          conv3d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, None, 120, 12 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_2 (Conv3D)               (None, None, 120, 12 2320        re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, 120, 12 64          conv3d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_2 (ReLU)                  (None, None, 120, 12 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_3 (Conv3D)               (None, None, 60, 60, 4640        re_lu_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 60, 60, 128         conv3d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_3 (ReLU)                  (None, None, 60, 60, 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_4 (Conv3D)               (None, None, 60, 60, 9248        re_lu_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, 60, 60, 128         conv3d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_4 (ReLU)                  (None, None, 60, 60, 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_5 (Conv3D)               (None, None, 30, 30, 13872       re_lu_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, 30, 30, 192         conv3d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_5 (ReLU)                  (None, None, 30, 30, 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_6 (Conv3D)               (None, None, 30, 30, 20784       re_lu_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, 30, 30, 192         conv3d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_6 (ReLU)                  (None, None, 30, 30, 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_7 (Conv3D)               (None, None, 15, 15, 27712       re_lu_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, 15, 15, 256         conv3d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_7 (ReLU)                  (None, None, 15, 15, 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_8 (Conv3D)               (None, None, 15, 15, 36928       re_lu_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, 15, 15, 256         conv3d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_8 (ReLU)                  (None, None, 15, 15, 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose (Conv3DTranspo (None, None, 30, 30, 27696       re_lu_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_9 (Conv3D)               (None, None, 30, 30, 20784       re_lu_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, 30, 30, 192         conv3d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, 30, 30, 192         conv3d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_9 (ReLU)                  (None, None, 30, 30, 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_10 (ReLU)                 (None, None, 30, 30, 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, None, 30, 30, 0           re_lu_9[0][0]                    \n",
            "                                                                 re_lu_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_10 (Conv3D)              (None, None, 30, 30, 20784       tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_11 (Conv3D)              (None, None, 60, 60, 9248        re_lu_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, None, 30, 30, 192         conv3d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, 60, 60, 128         conv3d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_11 (ReLU)                 (None, None, 30, 30, 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_13 (ReLU)                 (None, None, 60, 60, 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_1 (Conv3DTrans (None, None, 60, 60, 13856       re_lu_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, None, 60, 60, 9248        re_lu_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, 60, 60, 128         conv3d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, None, 60, 60, 128         conv3d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_14 (Conv3D)              (None, None, 120, 12 2320        re_lu_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_12 (ReLU)                 (None, None, 60, 60, 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_14 (ReLU)                 (None, None, 60, 60, 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, None, 120, 12 64          conv3d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, None, 60, 60, 0           re_lu_12[0][0]                   \n",
            "                                                                 re_lu_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_17 (ReLU)                 (None, None, 120, 12 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_13 (Conv3D)              (None, None, 60, 60, 9248        tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_15 (Conv3D)              (None, None, 120, 12 2320        re_lu_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, 60, 60, 128         conv3d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, None, 120, 12 64          conv3d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_15 (ReLU)                 (None, None, 60, 60, 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_18 (ReLU)                 (None, None, 120, 12 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_2 (Conv3DTrans (None, None, 120, 12 4624        re_lu_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_16 (Conv3D)              (None, None, 120, 12 2320        re_lu_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, None, 120, 12 64          conv3d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, None, 120, 12 64          conv3d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_18 (Conv3D)              (None, None, 240, 24 584         re_lu[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_16 (ReLU)                 (None, None, 120, 12 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_19 (ReLU)                 (None, None, 120, 12 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, None, 240, 24 32          conv3d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_2 (TFOpLam (None, None, 120, 12 0           re_lu_16[0][0]                   \n",
            "                                                                 re_lu_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_22 (ReLU)                 (None, None, 240, 24 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_17 (Conv3D)              (None, None, 120, 12 2320        tf.__operators__.add_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_19 (Conv3D)              (None, None, 240, 24 584         re_lu_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, None, 120, 12 64          conv3d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, None, 240, 24 32          conv3d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_20 (ReLU)                 (None, None, 120, 12 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_23 (ReLU)                 (None, None, 240, 24 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_3 (Conv3DTrans (None, None, 240, 24 1160        re_lu_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_20 (Conv3D)              (None, None, 240, 24 584         re_lu_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, None, 240, 24 32          conv3d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, None, 240, 24 32          conv3d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_21 (ReLU)                 (None, None, 240, 24 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_24 (ReLU)                 (None, None, 240, 24 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_3 (TFOpLam (None, None, 240, 24 0           re_lu_21[0][0]                   \n",
            "                                                                 re_lu_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_21 (Conv3D)              (None, None, 240, 24 584         tf.__operators__.add_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, None, 240, 24 32          conv3d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_25 (ReLU)                 (None, None, 240, 24 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_22 (Conv3D)              (None, None, 240, 24 584         re_lu_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, None, 240, 24 32          conv3d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_26 (ReLU)                 (None, None, 240, 24 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tumor (Conv3D)                  (None, None, 240, 24 146         re_lu_26[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 248,874\n",
            "Trainable params: 247,418\n",
            "Non-trainable params: 1,456\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnTQBsKVtOa8"
      },
      "source": [
        "def dice(y_true, y_pred, c=1, epsilon=1):\n",
        "    \"\"\"\n",
        "    Method to calculate the Dice score coefficient for given class\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (np.ndarray) y_true : ground-truth label\n",
        "      (np.ndarray) y_pred : predicted logits scores\n",
        "      (int)             c : class to calculate DSC on\n",
        "    \n",
        "    \"\"\"\n",
        "    assert y_true.ndim == y_pred.ndim\n",
        "    \n",
        "    true = y_true[..., 0] == c\n",
        "    pred = np.argmax(y_pred, axis=-1) == c \n",
        "\n",
        "    A = np.count_nonzero(true & pred) * 2\n",
        "    B = np.count_nonzero(true) + np.count_nonzero(pred) + epsilon\n",
        "    \n",
        "    return A / B\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jfNxJNpC3VO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4f69c1-9fda-4eae-8de4-e55aa210f3fc"
      },
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "\n",
        "dsc_valid = []\n",
        "dsc_train=[]\n",
        "\n",
        "for x, y in test_train:\n",
        "    \n",
        "    # --- Predict\n",
        "    logits = model.predict(x['dat'])\n",
        "\n",
        "    if type(logits) is dict:\n",
        "        logits = logits['tumor']\n",
        "\n",
        "    # --- Argmax\n",
        "    dsc_train.append(dice(y['tumor'][0], logits[0], c=1))\n",
        "\n",
        "dsc_train = np.array(dsc_train)\n",
        "\n",
        "for x, y in test_valid:\n",
        "   # --- Predict\n",
        "    logits = model.predict(x['dat'])\n",
        "\n",
        "    if type(logits) is dict:\n",
        "        logits = logits['tumor']\n",
        "\n",
        "    # --- Argmax\n",
        "    dsc_valid.append(dice(y['tumor'][0], logits[0], c=1))\n",
        "dsc_valid = np.array(dsc_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-14 16:53:01 ] [====================] 100.000% : Iterating | 000074    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU44s19bfSO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eec4a92-b672-4141-d765-07a061ce86ee"
      },
      "source": [
        "print(np.mean(dsc_train), np.mean(dsc_valid), np.median(dsc_valid), np.median(dsc_train), np.quantile(dsc_valid, 0.75), np.quantile(dsc_train, 0.75))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8302406021197961 0.7980558647745607 0.884943291022225 0.8788957909323272 0.9127488337803554 0.9156923394696403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWgnXQZT9WIE"
      },
      "source": [
        "# --- Define columns\n",
        "df_train = pd.DataFrame(index=np.arange(dsc_train.size))\n",
        "df_valid=pd.DataFrame(index=np.arange(dsc_valid.size))\n",
        "df_stats=pd.DataFrame(index=np.arange(1))\n",
        "\n",
        "df_train['Dice score Training']=dsc_train\n",
        "df_valid['Dice score Validation']=dsc_valid\n",
        "\n",
        "df_stats['Dice score Training Median'] = df_train['Dice score Training'].median()\n",
        "df_stats['Dice score Training Mean'] = df_train['Dice score Training'].mean()\n",
        "df_stats['Dice score Training 25th percentile: '] = df_train['Dice score Training'].quantile(0.25)\n",
        "df_stats['Dice score Training 75th percentile: '] = df_train['Dice score Training'].quantile(0.75)\n",
        "\n",
        "df_stats['Dice score Validation Median'] = df_valid['Dice score Validation'].median()\n",
        "df_stats['Dice score Validation Mean'] = df_valid['Dice score Validation'].mean()\n",
        "df_stats['Dice score Validation 25th percentile: '] = df_valid['Dice score Validation'].quantile(0.25)\n",
        "df_stats['Dice score Validation 75th percentile: '] = df_valid['Dice score Validation'].quantile(0.75)\n",
        "\n",
        "# --- Print accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbm1AzSVe9Yr"
      },
      "source": [
        "df_train.to_csv('./train_results_m1.csv')\n",
        "df_valid.to_csv('./valid_results_m1.csv')\n",
        "df_stats.to_csv('./results_summary.csv')\n",
        "model.save('./model_1.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHwBEDpPayu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c594e3b8-c685-4d44-da71-f1846170a524"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMK77x2SMtS9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJnCPRMKrA3t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn2G2adOMx4r"
      },
      "source": [
        "## 2. Hybrid 3D/2D U-Net\n",
        "\n",
        "In this algorithm, the original 2D model is modified to yield a hybrid 3D/2D model that uses several continguous slices for prediction of any given single output slice. The network is **identical** to the architecture presented in the week 6 tutorials. To ensure a fair comparison, recommend using similar network design and training hyperparameters as in the first 2D only model above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Miu1DQX-Mx4s"
      },
      "source": [
        "### Create generators and inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7TK7jFMMx4s"
      },
      "source": [
        "# --- Input ==> 3 x 240 x 240 x 4\n",
        "configs = {\n",
        "    'specs': {\n",
        "        'xs': {'dat': {'shape': [3, 240, 240, 4]}},\n",
        "        'ys': {'tumor': {'norms': {'clip': {'max': 1}}}}}}\n",
        "\n",
        "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-mip', keyword='mip*vox', configs=configs)\n",
        "inputs = client.get_inputs(Input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bes4odYHqx7J"
      },
      "source": [
        "# --- Define 2D conv (xy-features)\n",
        "conv_2d = lambda x, filters, strides : layers.Conv3D(\n",
        "    filters=filters, \n",
        "    strides=strides, \n",
        "    kernel_size=(1, 3, 3), \n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "# --- Define 1D conv (z-features)\n",
        "conv_1d = lambda x, filters, k=2 : layers.Conv3D(\n",
        "    filters=filters,\n",
        "    strides=1,\n",
        "    kernel_size=(k, 1, 1),\n",
        "    padding='valid',\n",
        "    kernel_initializer='he_normal')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIGGa2MPqzeg"
      },
      "source": [
        "# --- Define lambda functions\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.LeakyReLU()(x)\n",
        "\n",
        "# --- Define stride-1 3D, stride-2 3D and stride-1 1D (z-subsample) blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv_2d(x, filters, strides=(1, 1, 1))))\n",
        "conv2 = lambda filters, x : relu(norm(conv_2d(x, filters, strides=(1, 2, 2))))\n",
        "convZ = lambda filters, k, x : relu(norm(conv_1d(x, filters, k=k)))\n",
        "\n",
        "# --- Define arbitrary input\n",
        "dat = Input(shape=(3, 240, 240, 4))\n",
        "\n",
        "# --- Define contracting layers\n",
        "l1 = conv1(8,  inputs['dat'])\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, convZ(48, 2, conv2(48, l3)))\n",
        "l5 = conv1(64, convZ(64, 2, conv2(64, l4)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viyLnyTzq2--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba580c3-c5dc-47ea-d47c-f2b04507a351"
      },
      "source": [
        "Model(inputs=inputs, outputs=l5)({'dat': dat})\n",
        "\n",
        "# --- 3-slices to 1-slice\n",
        "p3 = convZ(32, 3, l3)\n",
        "print(Model(inputs=inputs, outputs=l3)({'dat': dat}).shape)\n",
        "print(Model(inputs=inputs, outputs=p3)({'dat': dat}).shape)\n",
        "\n",
        "# --- 2-slices to 1-slice\n",
        "p4 = convZ(48, 2, l4)\n",
        "print(Model(inputs=inputs, outputs=l4)({'dat': dat}).shape)\n",
        "print(Model(inputs=inputs, outputs=p4)({'dat': dat}).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 3, 60, 60, 32)\n",
            "(None, 1, 60, 60, 32)\n",
            "(None, 2, 30, 30, 48)\n",
            "(None, 1, 30, 30, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2cykiRoriVh"
      },
      "source": [
        "# --- Define 2D transpose\n",
        "tran = lambda x, filters : layers.Conv3DTranspose(\n",
        "    filters=filters, \n",
        "    strides=(1, 2, 2),\n",
        "    kernel_size=(1, 3, 3),\n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "# --- Define transpose block\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF72nDKkq9aa"
      },
      "source": [
        "# --- Create expanding layers using concatenation\n",
        "concat = lambda a, b : layers.Concatenate()([a, b])\n",
        "\n",
        "l6 =  tran2(48, conv1(48, l5))\n",
        "l7 =  tran2(32, conv1(48, concat(convZ(48, 2, l4), l6)))\n",
        "l8 =  tran2(16, conv1(32, concat(convZ(32, 3, l3), l7)))\n",
        "l9 =  tran2(8,  conv1(16, concat(convZ(16, 3, l2), l8)))\n",
        "l10 = conv1(8,  conv1(8,  concat(convZ(8,  3, l1), l9)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMjtbvOarFGy"
      },
      "source": [
        "# --- Create logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm-Zgm7GMx4s"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwl1WXtaMx4t"
      },
      "source": [
        "# --- Define model\n",
        "\n",
        "# --- Create logits\n",
        "logits = {}\n",
        "logits['tumor'] = layers.Conv3D(\n",
        "    name='tumor',\n",
        "    filters=2, \n",
        "    strides=1, \n",
        "    kernel_size=(1, 3, 3), \n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(l10)\n",
        "# --- Create model\n",
        "model = Model(inputs=inputs, outputs=logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD6ViRaRMx4t"
      },
      "source": [
        "### Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKyb21n9Mx4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c35b20-a896-453d-8560-1786a20b5874"
      },
      "source": [
        "# --- Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss={'tumor': losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
        "    metrics={'tumor': custom.dsc(cls=1)},\n",
        "    experimental_run_tf_function=False)\n",
        "# --- Train the model\n",
        "model.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=500, \n",
        "    epochs=12,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=500,\n",
        "    validation_freq=4,\n",
        "    use_multiprocessing=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 378s 734ms/step - loss: 0.5051 - dsc_1: 0.2951\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 365s 730ms/step - loss: 0.0440 - dsc_1: 0.7699\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 365s 731ms/step - loss: 0.0200 - dsc_1: 0.8089\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0141 - dsc_1: 0.8165WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 706s 1s/step - loss: 0.0141 - dsc_1: 0.8166 - val_loss: 0.0132 - val_dsc_1: 0.8304\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 363s 726ms/step - loss: 0.0112 - dsc_1: 0.8538\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 367s 733ms/step - loss: 0.0102 - dsc_1: 0.8367\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 364s 728ms/step - loss: 0.0091 - dsc_1: 0.8596\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0083 - dsc_1: 0.8770WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 692s 1s/step - loss: 0.0083 - dsc_1: 0.8770 - val_loss: 0.0103 - val_dsc_1: 0.8523\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 347s 695ms/step - loss: 0.0078 - dsc_1: 0.8563\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 356s 711ms/step - loss: 0.0079 - dsc_1: 0.8495\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 356s 712ms/step - loss: 0.0069 - dsc_1: 0.8765\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0072 - dsc_1: 0.8675WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 685s 1s/step - loss: 0.0072 - dsc_1: 0.8675 - val_loss: 0.0093 - val_dsc_1: 0.8579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb82557b790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKFTyNDGmxci"
      },
      "source": [
        "def dice(y_true, y_pred, c=1, epsilon=1):\n",
        "    \"\"\"\n",
        "    Method to calculate the Dice score coefficient for given class\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (np.ndarray) y_true : ground-truth label\n",
        "      (np.ndarray) y_pred : predicted logits scores\n",
        "      (int)             c : class to calculate DSC on\n",
        "    \n",
        "    \"\"\"\n",
        "    assert y_true.ndim == y_pred.ndim\n",
        "    \n",
        "    true = y_true[..., 0] == c\n",
        "    pred = np.argmax(y_pred, axis=-1) == c \n",
        "\n",
        "    A = np.count_nonzero(true & pred) * 2\n",
        "    B = np.count_nonzero(true) + np.count_nonzero(pred) + epsilon\n",
        "    \n",
        "    return A / B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daw3Uqfimz2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ed5e35-7f55-4898-afc3-e24c899ce3d8"
      },
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "\n",
        "dsc_train_m2 = []\n",
        "dsc_valid_m2=[]\n",
        "\n",
        "for x, y in test_train:\n",
        "    \n",
        "    # --- Predict\n",
        "    x['dat'] = np.pad(x['dat'], ((0, 0), (1, 1), (0, 0), (0, 0), (0, 0)))\n",
        "    logits = model.predict(x['dat'])\n",
        "\n",
        "    if type(logits) is dict:\n",
        "        logits = logits['tumor']\n",
        "\n",
        "    # --- Argmax\n",
        "    dsc_train_m2.append(dice(y['tumor'][0], logits[0], c=1))\n",
        "    \n",
        "dsc_train_m2 = np.array(dsc_train_m2)\n",
        "\n",
        "\n",
        "for x, y in test_valid:\n",
        "    \n",
        "    # --- Predict\n",
        "    x['dat'] = np.pad(x['dat'], ((0, 0), (1, 1), (0, 0), (0, 0), (0, 0)))\n",
        "    logits = model.predict(x['dat'])\n",
        "\n",
        "    if type(logits) is dict:\n",
        "        logits = logits['tumor']\n",
        "\n",
        "    0# --- Argmax\n",
        "    dsc_valid_m2.append(dice(y['tumor'][0], logits[0], c=1))\n",
        "    \n",
        "dsc_valid_m2 = np.array(dsc_valid_m2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-14 18:31:30 ] [====================] 100.000% : Iterating | 000074    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bhz8D-TNkDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4094dc62-ea17-459e-ac3b-306e68fa9a88"
      },
      "source": [
        "model.summary()\n",
        "df_train.to_csv('./train_results_m2.csv')\n",
        "df_valid.to_csv('./valid_results_m2.csv')\n",
        "model.save('./model_2.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "dat (InputLayer)                [(None, None, 240, 2 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_23 (Conv3D)              (None, None, 240, 24 296         dat[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, None, 240, 24 32          conv3d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, None, 240, 24 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_24 (Conv3D)              (None, None, 120, 12 1168        leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, None, 120, 12 64          conv3d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, None, 120, 12 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_25 (Conv3D)              (None, None, 120, 12 2320        leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, None, 120, 12 64          conv3d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, None, 120, 12 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_26 (Conv3D)              (None, None, 60, 60, 4640        leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, None, 60, 60, 128         conv3d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, None, 60, 60, 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_27 (Conv3D)              (None, None, 60, 60, 9248        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, None, 60, 60, 128         conv3d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, None, 60, 60, 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_28 (Conv3D)              (None, None, 30, 30, 13872       leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, None, 30, 30, 192         conv3d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, None, 30, 30, 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_29 (Conv3D)              (None, None, 30, 30, 4656        leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, None, 30, 30, 192         conv3d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, None, 30, 30, 0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_30 (Conv3D)              (None, None, 30, 30, 20784       leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, None, 30, 30, 192         conv3d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, None, 30, 30, 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_31 (Conv3D)              (None, None, 15, 15, 27712       leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, None, 15, 15, 256         conv3d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, None, 15, 15, 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_32 (Conv3D)              (None, None, 15, 15, 8256        leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, None, 15, 15, 256         conv3d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, None, 15, 15, 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_33 (Conv3D)              (None, None, 15, 15, 36928       leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, None, 15, 15, 256         conv3d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, None, 15, 15, 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_36 (Conv3D)              (None, None, 15, 15, 27696       leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, None, 15, 15, 192         conv3d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, None, 15, 15, 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_37 (Conv3D)              (None, None, 30, 30, 4656        leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_4 (Conv3DTrans (None, None, 30, 30, 20784       leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, None, 30, 30, 192         conv3d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, None, 30, 30, 192         conv3d_transpose_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, None, 30, 30, 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, None, 30, 30, 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, 30, 30, 0           leaky_re_lu_15[0][0]             \n",
            "                                                                 leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_38 (Conv3D)              (None, None, 30, 30, 41520       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, None, 30, 30, 192         conv3d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, None, 30, 30, 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_39 (Conv3D)              (None, None, 60, 60, 3104        leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_5 (Conv3DTrans (None, None, 60, 60, 13856       leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, None, 60, 60, 128         conv3d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, None, 60, 60, 128         conv3d_transpose_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, None, 60, 60, 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, None, 60, 60, 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, 60, 60, 0           leaky_re_lu_18[0][0]             \n",
            "                                                                 leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_40 (Conv3D)              (None, None, 60, 60, 18464       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, None, 60, 60, 128         conv3d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, None, 60, 60, 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_41 (Conv3D)              (None, None, 120, 12 784         leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_6 (Conv3DTrans (None, None, 120, 12 4624        leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, None, 120, 12 64          conv3d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, None, 120, 12 64          conv3d_transpose_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, None, 120, 12 0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, None, 120, 12 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, 120, 12 0           leaky_re_lu_21[0][0]             \n",
            "                                                                 leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_42 (Conv3D)              (None, None, 120, 12 4624        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, None, 120, 12 64          conv3d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, None, 120, 12 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_43 (Conv3D)              (None, None, 240, 24 200         leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_7 (Conv3DTrans (None, None, 240, 24 1160        leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, None, 240, 24 32          conv3d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, None, 240, 24 32          conv3d_transpose_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)      (None, None, 240, 24 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, None, 240, 24 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, None, 240, 24 0           leaky_re_lu_24[0][0]             \n",
            "                                                                 leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_44 (Conv3D)              (None, None, 240, 24 1160        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, None, 240, 24 32          conv3d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)      (None, None, 240, 24 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_45 (Conv3D)              (None, None, 240, 24 584         leaky_re_lu_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, None, 240, 24 32          conv3d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_26 (LeakyReLU)      (None, None, 240, 24 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tumor (Conv3D)                  (None, None, 240, 24 146         leaky_re_lu_26[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 276,474\n",
            "Trainable params: 274,858\n",
            "Non-trainable params: 1,616\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJfqhDU7nmwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e692b715-1b45-4703-88eb-7dea8e2c57f5"
      },
      "source": [
        "print(np.mean(dsc_train_m2), np.mean(dsc_valid_m2), \n",
        "      np.median(dsc_valid_m2), np.median(dsc_train_m2), \n",
        "      np.quantile(dsc_valid_m2, 0.75), np.quantile(dsc_train_m2, 0.75))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.884958877754717 0.845669669432368 0.893690591820192 0.9077242954767188 0.9266174131859266 0.9326295618325435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCl6jY0lMx4t"
      },
      "source": [
        "## 3. Custom architecture\n",
        "\n",
        "Finally, using any of the customizations described in class, find a top-performing model that yields some incremental benefit over the two baseline models above. Modifications that may be used include (but are not limited to):\n",
        "\n",
        "* hybrid 3D/2D network\n",
        "* residual connections\n",
        "* added convolutions between contracting and expanding layers \n",
        "* modifications to the convolutional blocks including ResNet, Inception, SE-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MoP1kZyMx4u"
      },
      "source": [
        "# --- Choose input (may copy the generator code from above)\n",
        "# --- Input ==> 3 x 240 x 240 x 4\n",
        "configs = {\n",
        "    'specs': {\n",
        "        'xs': {'dat': {'shape': [3, 240, 240, 4]}},\n",
        "        'ys': {'tumor': {'norms': {'clip': {'max': 1}}}}}}\n",
        "\n",
        "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-mip', keyword='mip*vox', configs=configs)\n",
        "inputs = client.get_inputs(Input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qSeqMeF1FLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25b1ea8-02f3-4d96-d7f0-66ef4bb34df9"
      },
      "source": [
        "# --- Define 2D conv (xy-features)\n",
        "conv_2d = lambda x, filters, strides : layers.Conv3D(\n",
        "    filters=filters, \n",
        "    strides=strides, \n",
        "    kernel_size=(1, 3, 3), \n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "# --- Define 1D conv (z-features)\n",
        "conv_1d = lambda x, filters, k=2 : layers.Conv3D(\n",
        "    filters=filters,\n",
        "    strides=1,\n",
        "    kernel_size=(k, 1, 1),\n",
        "    padding='valid',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "    # --- Define lambda functions\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.LeakyReLU()(x)\n",
        "\n",
        "# --- Define stride-1 3D, stride-2 3D and stride-1 1D (z-subsample) blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv_2d(x, filters, strides=(1, 1, 1))))\n",
        "conv2 = lambda filters, x : relu(norm(conv_2d(x, filters, strides=(1, 2, 2))))\n",
        "convZ = lambda filters, k, x : relu(norm(conv_1d(x, filters, k=k)))\n",
        "\n",
        "# --- Define arbitrary input\n",
        "dat = Input(shape=(3, 240, 240, 4))\n",
        "\n",
        "# --- Define contracting layers\n",
        "l1 = conv1(8,  inputs['dat'])\n",
        "\n",
        "# --- Squeeze (global pool)\n",
        "p1 = layers.GlobalAveragePooling3D()(l1)\n",
        "\n",
        "# --- Excitation (reduce channels to 1 / R) ==> in this example set R = 4 arbitrarily\n",
        "ch = int(p1.shape[-1] / 4)\n",
        "f1 = layers.Dense(ch, activation='relu')(p1)\n",
        "\n",
        "# --- Scale (expand channels to original size)\n",
        "scale = layers.Dense(l1.shape[-1], activation='sigmoid')(f1)\n",
        "scale = layers.Reshape((1, 1, 1, l1.shape[-1]))(scale)    \n",
        "\n",
        "# --- Modify l1\n",
        "l1 = l1 * scale\n",
        "\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, convZ(48, 2, conv2(48, l3)))\n",
        "l5 = conv1(64, convZ(64, 2, conv2(64, l4)))\n",
        "\n",
        "\n",
        "Model(inputs=inputs, outputs=l5)({'dat': dat})\n",
        "\n",
        "# --- 3-slices to 1-slice\n",
        "p3 = convZ(32, 3, l3)\n",
        "print(Model(inputs=inputs, outputs=l3)({'dat': dat}).shape)\n",
        "print(Model(inputs=inputs, outputs=p3)({'dat': dat}).shape)\n",
        "\n",
        "# --- 2-slices to 1-slice\n",
        "p4 = convZ(48, 2, l4)\n",
        "print(Model(inputs=inputs, outputs=l4)({'dat': dat}).shape)\n",
        "print(Model(inputs=inputs, outputs=p4)({'dat': dat}).shape)\n",
        "# --- Define 2D transpose\n",
        "\n",
        "tran = lambda x, filters : layers.Conv3DTranspose(\n",
        "    filters=filters, \n",
        "    strides=(1, 2, 2),\n",
        "    kernel_size=(1, 3, 3),\n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "# --- Define transpose block\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters)))\n",
        "\n",
        "# --- Create expanding layers using concatenation\n",
        "concat = lambda a, b : layers.Concatenate()([a, b])\n",
        "\n",
        "l6 =  tran2(48, conv1(48, l5))\n",
        "l7 =  tran2(32, conv1(48, concat(convZ(48, 2, l4), l6)))\n",
        "l8 =  tran2(16, conv1(32, concat(convZ(32, 3, l3), l7)))\n",
        "l9 =  tran2(8,  conv1(16, concat(convZ(16, 3, l2), l8)))\n",
        "l10 = conv1(8,  conv1(8,  concat(convZ(8,  3, l1), l9)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 3, 60, 60, 32)\n",
            "(None, 1, 60, 60, 32)\n",
            "(None, 2, 30, 30, 48)\n",
            "(None, 1, 30, 30, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN6yMSd-Mx4u"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1XLPD8OmYWk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Me0xkeYMx4u"
      },
      "source": [
        "# --- Define model\n",
        "\n",
        "# --- Create logits\n",
        "logits = {}\n",
        "logits['tumor'] = layers.Conv3D(\n",
        "    name='tumor',\n",
        "    filters=2, \n",
        "    strides=1, \n",
        "    kernel_size=(1, 3, 3), \n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(l10)\n",
        "\n",
        "# --- Create model\n",
        "model = Model(inputs=inputs, outputs=logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssopud5KMx4u"
      },
      "source": [
        "### Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLwICHOkMx4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639a899d-7411-40c9-a3cd-0ba0e2cb22ac"
      },
      "source": [
        "# --- Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss={'tumor': losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
        "    metrics={'tumor': custom.dsc(cls=1)},\n",
        "    experimental_run_tf_function=False)\n",
        "# --- Train the model\n",
        "model.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=500, \n",
        "    epochs=12,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=500,\n",
        "    validation_freq=4,\n",
        "    use_multiprocessing=True)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 370s 726ms/step - loss: 0.2414 - dsc_1: 0.3876\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 364s 728ms/step - loss: 0.0250 - dsc_1: 0.7784\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 360s 720ms/step - loss: 0.0142 - dsc_1: 0.8304\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0113 - dsc_1: 0.8541WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 707s 1s/step - loss: 0.0113 - dsc_1: 0.8541 - val_loss: 0.0113 - val_dsc_1: 0.8320\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 363s 727ms/step - loss: 0.0094 - dsc_1: 0.8537\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 367s 733ms/step - loss: 0.0093 - dsc_1: 0.8668\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 364s 728ms/step - loss: 0.0083 - dsc_1: 0.8543\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0078 - dsc_1: 0.8588WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 708s 1s/step - loss: 0.0078 - dsc_1: 0.8588 - val_loss: 0.0097 - val_dsc_1: 0.8455\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 364s 730ms/step - loss: 0.0077 - dsc_1: 0.8715\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 367s 733ms/step - loss: 0.0073 - dsc_1: 0.8745\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 368s 736ms/step - loss: 0.0069 - dsc_1: 0.8713\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.0068 - dsc_1: 0.8741WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "500/500 [==============================] - 709s 1s/step - loss: 0.0068 - dsc_1: 0.8742 - val_loss: 0.0098 - val_dsc_1: 0.8510\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb81aec86d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r6GCJqIQDB-"
      },
      "source": [
        "def dice(y_true, y_pred, c=1, epsilon=1):\n",
        "    \"\"\"````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
        "    Method to calculate the Dice score coefficient for given class\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (np.ndarray) y_true : ground-truth label\n",
        "      (np.ndarray) y_pred : predicted logits scores\n",
        "      (int)             c : class to calculate DSC on\n",
        "    \n",
        "    \"\"\"\n",
        "    assert y_true.ndim == y_pred.ndim\n",
        "    \n",
        "    true = y_true[..., 0] == c\n",
        "    pred = np.argmax(y_pred, axis=-1) == c `````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
        "\n",
        "    A = np.count_nonzero(true & pred) * 2\n",
        "    B = np.count_nonzero(true) + np.count_nonzero(pred) + epsilon\n",
        "    \n",
        "    return A / B"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGED6UvwhHp9",
        "outputId": "8461b502-593f-4a70-ffa4-23658d905fc8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJY54SAwNuad"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uv1lULZMx4v"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "For each of the three models, the following metrics should be calculated for **both the training and validation** cohorts:\n",
        "\n",
        "* Dice score, mean\n",
        "* Dice score, median\n",
        "* Dice score, 25th percentile\n",
        "* Dice score, 75th percentile\n",
        "\n",
        "As in prior assignments, accuracy is determined on a patient by patient (volume by volume) basis, so please calculate the Dice score values on the entire 3D volume (not slice-by-slice).\n",
        "\n",
        "### Performance\n",
        "\n",
        "The following minimum performance metrics must be met for full credit:\n",
        "\n",
        "1. **2D U-Net**: median Dice score > 0.64\n",
        "2. **Hybrid 3D/2D U-Net**: median Dice score > 0.68\n",
        "3. **Custom architecture**: median Dice score > 0.72"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvJ-7xw5Mx4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e2d296-a44e-49f7-f0bd-fcfaa71e9c54"
      },
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "\n",
        "dsc_train_m3 = []\n",
        "dsc_valid_m3=[]\n",
        "\n",
        "for x, y in test_train:\n",
        "    \n",
        "    # --- Predict\n",
        "    x['dat'] = np.pad(x['dat'], ((0, 0), (1, 1), (0, 0), (0, 0), (0, 0)))\n",
        "    logits = model.predict(x['dat'])\n",
        "\n",
        "    if type(logits) is dict:\n",
        "        logits = logits['tumor']\n",
        "\n",
        "    # --- Argmax\n",
        "    dsc_train_m3.append(dice(y['tumor'][0], logits[0], c=1))\n",
        "    \n",
        "dsc_train_m3 = np.array(dsc_train_m3)\n",
        "\n",
        "\n",
        "for x, y in test_valid:\n",
        "    \n",
        "    # --- Predict\n",
        "    x['dat'] = np.pad(x['dat'], ((0, 0), (1, 1), (0, 0), (0, 0), (0, 0)))\n",
        "    logits = model.predict(x['dat'])\n",
        "\n",
        "    if type(logits) is dict:\n",
        "        logits = logits['tumor']\n",
        "\n",
        "    # --- Argmax\n",
        "    dsc_valid_m3.append(dice(y['tumor'][0], logits[0], c=1))\n",
        "    \n",
        "dsc_valid_m3 = np.array(dsc_valid_m3)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-14 20:11:03 ] [====================] 100.000% : Iterating | 000074    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME5kka5KN1zY"
      },
      "source": [
        "print(np.mean(dsc_train_m3), np.mean(dsc_valid_m3), \n",
        "      np.median(dsc_valid_m3), np.median(dsc_train_m3), \n",
        "      np.quantile(dsc_valid_m3, 0.75), np.quantile(dsc_train_m3, 0.75))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ7UFuZ-Mx4v"
      },
      "source": [
        "### Results\n",
        "\n",
        "When ready, create a `*.csv` file with your compiled **training and validation** cohort statistics for the three different models. Consider the following table format (although any format that contains the required information is sufficient):\n",
        "\n",
        "```\n",
        "          TRAINING                                VALIDATION\n",
        "          mean | median | 25th-tile | 75th-tile | mean | median | 25th-tile | 75th-tile\n",
        "model 1\n",
        "model 2\n",
        "model 3\n",
        "```\n",
        "\n",
        "As above, statistics for both training and validation should be provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnJMbNrSMx4v"
      },
      "source": [
        "# --- Create *.csv\n",
        "df_results = pd.DataFrame(columns=['train mean', 'train median', 'train 25th percentile', 'train 75th percentiele', 'valid mean', 'valid median', 'valid 25th percentile', 'valid 75th percentiele'])\n",
        "df_results.loc[0]=np.mean(dsc_train), np.median(dsc_train), np.quantile(dsc_train, 0.25), np.quantile(dsc_train, 0.75), np.mean(dsc_valid), np.median(dsc_valid), np.quantile(dsc_valid, 0.25), np.quantile(dsc_valid, 0.75)\n",
        "df_results.loc[1]=np.mean(dsc_train_m2), np.median(dsc_train_m2), np.quantile(dsc_train_m2, 0.25), np.quantile(dsc_train_m2, 0.75), np.mean(dsc_valid_m2), np.median(dsc_valid_m2), np.quantile(dsc_valid_m2, 0.25), np.quantile(dsc_valid_m2, 0.75)\n",
        "df_results.loc[2]=np.mean(dsc_train_m3), np.median(dsc_train_m3), np.quantile(dsc_train_m3, 0.25), np.quantile(dsc_train_m3, 0.75), np.mean(dsc_valid_m3), np.median(dsc_valid_m3), np.quantile(dsc_valid_m3, 0.25), np.quantile(dsc_valid_m3, 0.75)\n",
        "                              \n",
        "# --- Serialize *.csv\n",
        "fname = './results.csv'\n",
        "df_results.to_csv(fname)\n",
        "df_train.to_csv('./train_results_m3.csv')\n",
        "df_valid.to_csv('./valid_results_m3.csv')\n",
        "model.save('./model_3.hdf5')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjNSSKWgMx4w"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In addition to algorithm training as above, a 1-2 page write-up is required for this project. The goal is to *briefly* summarize algorithm design and key results. The write-up should be divided into three sections: methods; results; discussion. More detailed information and tips can be found here: https://github.com/peterchang77/dl_tutor/blob/master/cs190/spring_2021/notebooks/midterm/checklist.md.\n",
        "\n",
        "### Methods\n",
        "\n",
        "In this section, include details such as:\n",
        "\n",
        "* **Data**: How much data was used. How many cases were utilized for training and validation?\n",
        "* **Network design**: What are the different network architectures? How many layers and parameters? Were 2D or 3D operations used? Recall that the `model.summary(...)` can be used to provide key summary statistics for this purpose. If desired, feel free to include a model figure or diagram.\n",
        "* **Implementation**: How was training implemented. What are the key hyperparameters (e.g. learning rate, batch size, optimizer, etc)? How many training iterations were required for convergence? Did these hyperparameters change during the course of training?\n",
        "* **Statistics**: What statistics do you plan to use to evaluate model accuracy? \n",
        "\n",
        "### Results\n",
        "\n",
        "In this section, briefly summarize experimental results (a few sentences), and include the result table(s) as derived above.\n",
        "\n",
        "### Discussion\n",
        "\n",
        "Were the results expected or unexpected? What accounts for the differences in performance between the algorithms? How did you choose the network architecture implemented in your final model? Feel free to elaborate on any additional observations noted during the course of this expierment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dk4FYIJMx4w"
      },
      "source": [
        "# Submission\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWpySj97Mx4w"
      },
      "source": [
        "### Canvas\n",
        "\n",
        "Once you have completed the midterm assignment, download the necessary files from Google Colab and your Google Drive. As in prior assigments, be sure to prepare:\n",
        "\n",
        "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
        "* final (results) spreadsheet: `[UCInetID]_results.csv` (compiled for all three parts)\n",
        "* final (trained) model: `[UCInetID]_model.hdf5` (three separate files for all three parts)\n",
        "\n",
        "In addition, submit the summary write-up as in any common document format (`.docx`, `.tex`, `.pdf`, etc):\n",
        "\n",
        "* final summary write-up: `[UCInetID]_summary.[docx|tex|pdf]`\n",
        "\n",
        "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooke file would be submitted under the name `panteater_notebook.ipynb`, his spreadsheet would be submitted under the name `panteater_results.csv` and and his model file would be submitted under the name `panteater_model.hdf5`."
      ]
    }
  ]
}