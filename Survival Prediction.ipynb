{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "Fnaghman_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEY60YA9sq8K"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "In this assignment we will build an unsupervised CNN autoencoder network. Subsequently, the pretrained encoding (contracting) backbone of the autoencoder will be used to create a model for survival prediction in brain tumor patients.\n",
        "\n",
        "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_A-EmdLsq8O"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Once complete, the following items must be submitted:\n",
        "\n",
        "* final `*.ipynb` notebook\n",
        "* final trained `*.hdf5` model file\n",
        "* final compiled `*.csv` file with performance statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56d3oMiMw8Wm"
      },
      "source": [
        "# Google Colab\n",
        "\n",
        "The following lines of code will configure your Google Colab environment for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnioU6V-sq8P"
      },
      "source": [
        "### Enable GPU runtime\n",
        "\n",
        "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
        "\n",
        "```\n",
        "Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMaL5-Zfsq8Q"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaMyHyQtsq8R"
      },
      "source": [
        "### Jarvis library\n",
        "\n",
        "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-07L-WEsq8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38d8c78-5228-4cec-ee62-1a79852f74a7"
      },
      "source": [
        "# --- Install jarvis (only in Google Colab or local runtime)\n",
        "% pip install jarvis-md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jarvis-md\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/8c/c0e9a5cc4840e50d0743824996f84a95922c4e21a71a991572323328df9e/jarvis_md-0.0.1a14-py3-none-any.whl (81kB)\n",
            "\r\u001b[K     |████                            | 10kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 21.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 30kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51kB 31.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 61kB 26.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 71kB 24.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (2.23.0)\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->jarvis-md) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->jarvis-md) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2020.12.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (2.4.7)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->jarvis-md) (1.5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->jarvis-md) (1.15.0)\n",
            "Installing collected packages: pyyaml, jarvis-md\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed jarvis-md-0.0.1a14 pyyaml-5.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhptwSD5sq8T"
      },
      "source": [
        "### Imports\n",
        "\n",
        "Use the following lines to import any additional needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZlrcOjxsq8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70aea601-7993-4300-f518-9c87d2290748"
      },
      "source": [
        "import numpy as np, pandas as pd\n",
        "from tensorflow import losses, optimizers\n",
        "from tensorflow.keras import Input, Model, models, layers\n",
        "from jarvis.train import datasets\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g57tbnbUI6GnrFA6smuYyTb7oPP6FGyQzanHtTswezP9xoVQiXgZ6o\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsP1p8tgsq8V"
      },
      "source": [
        "# Data\n",
        "\n",
        "The data used in this assignment will consist of brain tumor MRI exams derived from the MICCAI Brain Tumor Segmentation Challenge (BRaTS). More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/. Each single 2D slice will consist of one of four different sequences (T2, FLAIR, T1 pre-contrast and T1 post-contrast)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XryXyvlVsq8V"
      },
      "source": [
        "The following lines of code will:\n",
        "\n",
        "1. Download the dataset (if not already present) \n",
        "2. Prepare the necessary Python generators to iterate through dataset\n",
        "3. Prepare the corresponding Tensorflow Input(...) objects for model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnQgSnbKsq8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718afe1d-0b28-406d-eef6-4c9847bd189a"
      },
      "source": [
        "# --- Download dataset\n",
        "datasets.download(name='mr/brats-2020-096')\n",
        "\n",
        "# --- Prepare generators and model inputs\n",
        "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-096', keyword='096*glb-org')\n",
        "inputs = client.get_inputs(Input)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-31 17:02:01 ] [====================] 100.000% : Extracting archive (0000486 / 0000486) "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baos-uGFsq8X"
      },
      "source": [
        "# Autoencoder\n",
        "\n",
        "In this assignment we will train a convolutional autoencoder. Compared to a standard contract-encoding U-Net architecture for semantic segmentation, two important distinctions should be emphasize:\n",
        "\n",
        "* no \"skip\" connections between the contractind and expanding layers\n",
        "* use of a regression loss function (e.g., MAE or MSE) for optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liyV9q5gsq8X"
      },
      "source": [
        "### Define model layers\n",
        "\n",
        "*Hint*: Recall that both a shared autoencoder and isolated encoder are needed to ensure that the contracting layers may be reused in a future model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNLMOHr7sq8Y"
      },
      "source": [
        "### --- Define kwargs dictionary\n",
        "kwargs = {\n",
        "    'kernel_size': (3, 3, 3),\n",
        "    'padding': 'same'}\n",
        "\n",
        "# --- Define lambda functions\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.ReLU()(x)\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        "\n",
        "# --- Define stride-1, stride-2 blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=2)))\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=2)))\n",
        "# --- Define contracting layers\n",
        "l1 = conv1(8, inputs['dat'])\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, conv2(48, l3))\n",
        "l5 = conv1(64, conv2(64, l4))\n",
        "\n",
        "# --- Define expanding layers\n",
        "l6  = tran2(48, l5)\n",
        "l7  = tran2(32, conv1(48, l6))\n",
        "l8  = tran2(16, conv1(32, l7))\n",
        "l9  = tran2(8,  conv1(16, l8))\n",
        "l10 = conv1(8,  l9)\n",
        "\n",
        "### --- Create autoencoder\n",
        "# --- Create autoencoder\n",
        "ae_outputs = {'recon': layers.Conv3D(filters=4, name='recon', **kwargs)(l10)}\n",
        "ae = Model(inputs=inputs, outputs=ae_outputs)\n",
        "# --- Create encoder\n",
        "# --- Create encoder\n",
        "encoder = Model(inputs=inputs, outputs=l5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri1YFPp8sq8Z"
      },
      "source": [
        "### Generator\n",
        "\n",
        "*Hint*: Recall that the default training generators which yield labels corresponding to survival scores need to be modified to instead yield the original input data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDawvHqRsq8Z"
      },
      "source": [
        "def ae_generator(G):\n",
        "    \n",
        "  for xs, ys in G:\n",
        "\n",
        "          ys = {'recon': xs['dat']}\n",
        "\n",
        "          yield xs, ys"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sX9a_Ntsq8a"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q89_kZJjsq8a"
      },
      "source": [
        "# --- Compile model\n",
        "ae.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
        "    loss={'recon': losses.MeanSquaredError()},\n",
        "    experimental_run_tf_function=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YobbsDE8sq8a"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s4MeYdIukPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb38d495-3d4e-451c-c117-ba466ca33476"
      },
      "source": [
        "client.load_data_in_memory()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2021-05-31 17:02:25 ] [====================] 100.000% : Iterating | 000235    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1GhFxfLsq8b"
      },
      "source": [
        "# --- Train model\n",
        "ae.fit(\n",
        "    x=ae_generator(gen_train), \n",
        "    steps_per_epoch=350, \n",
        "    epochs=8,\n",
        "    use_multiprocessing=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sZF3a8Rsq8b"
      },
      "source": [
        "# Survival Model\n",
        "\n",
        "In the second part of this assignment, you will create a dedicated survival prediction model using the pretrained encoder layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7GqjPVnsq8c"
      },
      "source": [
        "# --- Define model\n",
        "encoder.trainable = False\n",
        "latent = encoder(inputs)\n",
        "\n",
        "# --- Finalize model\n",
        "h0 = layers.Flatten()(latent)\n",
        "h1 = layers.Dense(32, activation='relu')(h0)\n",
        "\n",
        "logits = {}\n",
        "logits['survival'] = layers.Dense(1, activation='sigmoid', name='survival')(h1)\n",
        "\n",
        "# --- Create encoder\n",
        "model = Model(inputs=inputs, outputs=logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91bOHOrou0wI"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Lg7usisq8c"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLKwCfcTsq8d"
      },
      "source": [
        "# --- Compile model\n",
        "# --- Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss={'survival': losses.MeanSquaredError()},\n",
        "    experimental_run_tf_function=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUpkfyzlsq8d"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay6dFPcfsq8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0877a909-bedd-4e3a-be1a-9346302e3366"
      },
      "source": [
        "# --- Train model\n",
        "model.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=275, \n",
        "    epochs=8,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=275,\n",
        "    validation_freq=4,\n",
        "    use_multiprocessing=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "275/275 [==============================] - 39s 138ms/step - loss: 0.0113\n",
            "Epoch 2/8\n",
            "275/275 [==============================] - 38s 138ms/step - loss: 0.0033\n",
            "Epoch 3/8\n",
            "275/275 [==============================] - 38s 137ms/step - loss: 9.2601e-04\n",
            "Epoch 4/8\n",
            "275/275 [==============================] - 80s 292ms/step - loss: 1.2808e-04 - val_loss: 0.0057\n",
            "Epoch 5/8\n",
            "275/275 [==============================] - 37s 134ms/step - loss: 2.4610e-05\n",
            "Epoch 6/8\n",
            "275/275 [==============================] - 38s 138ms/step - loss: 5.3842e-05\n",
            "Epoch 7/8\n",
            "275/275 [==============================] - 38s 138ms/step - loss: 3.1227e-04\n",
            "Epoch 8/8\n",
            "275/275 [==============================] - 79s 288ms/step - loss: 2.9271e-04 - val_loss: 0.0060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f920f20c0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tpsZaGFsq8e"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Based on the tutorial discussion, use the following cells to calculate model performance. The following metrics should be calculated:\n",
        "\n",
        "* absolute error (mean, median, 25th percentile, 75th percentile)\n",
        "\n",
        "### Performance\n",
        "\n",
        "The following minimum performance metrics must be met for full credit:\n",
        "\n",
        "* median absolute error of < 0.09"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1qADBMDsq8f"
      },
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "mae = []\n",
        "\n",
        "for x, y in test_valid:\n",
        "    \n",
        "    # --- Predict\n",
        "    logits = model.predict(x['dat'])\n",
        "\n",
        "    if type(logits) is dict:\n",
        "        logits = logits['survival']\n",
        "\n",
        "    # --- Aggregate\n",
        "    preds.append(logits.ravel())\n",
        "    trues.append(y['survival'].ravel())\n",
        "    mae.append(np.abs(preds[-1] - trues[-1]))\n",
        "\n",
        "preds = np.array(preds).ravel()\n",
        "trues = np.array(trues).ravel()\n",
        "mae = np.array(mae).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSnfEd5Hsq8f"
      },
      "source": [
        "### Results\n",
        "\n",
        "When ready, create a `*.csv` file with your compiled **validation** cohort absolute prediction error statistics. There is no need to submit training performance accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hwqQdUrsq8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f986a640-84fa-4098-b556-ae3e3382d1bf"
      },
      "source": [
        "# --- Define columns\n",
        "df = pd.DataFrame(index=np.arange(mae.size))\n",
        "df['MAE'] = mae\n",
        "df.to_csv('./results.csv')\n",
        "model.save('./survival.hdf5')\n",
        "drive.mount('/content/drive')\n",
        "# --- Print accuracy\n",
        "print(df['MAE'].mean())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0.05868181213736534\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0.05868181213736534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDTw_ciQMh6A"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td7PC8Oysq8g"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Use the following line to save your model for submission:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnlYYL6isq8i"
      },
      "source": [
        "### Canvas\n",
        "\n",
        "Once you have completed this assignment, download the necessary files from Google Colab and your Google Drive. You will then need to submit the following items:\n",
        "\n",
        "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
        "* final (results) spreadsheet: `[UCInetID]_results.csv`\n",
        "* final (trained) model: `[UCInetID]_model.hdf5`\n",
        "\n",
        "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooke file would be submitted under the name `panteater_notebook.ipynb`, his spreadshhet would be submitted under the name `panteater_results.csv` and and his model file would be submitted under the name `panteater_model.hdf5`."
      ]
    }
  ]
}